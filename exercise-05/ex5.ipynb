{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57018dcf",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution. Please read all the instructions in each notebook carefully.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f7199d-c442-4732-ab89-83bda42682a7",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Exercise 5: Machine Translation with Transformers\n",
    "\n",
    "In this exercise, you will explore the Transformer-based neural architecture by tackling a machine translation task. Machine translation involves translating text from a source language to a target language. Traditionally, machine translation was performed using recurrent neural network (RNN)-based architectures. However, the emergence of Transformers has marked a revolutionary shift in the fields of text analysis and especially machine translation.\n",
    "\n",
    "Transformers offer several advantages over earlier RNN-based models for the machine translation task:\n",
    "\n",
    "* One major benefit is their ability to capture context from a long sequence using self-attention layers, which allows the model to retain relevant information from words further back in the text.\n",
    "  \n",
    "* Additionally, Transformers improve the natural flow and grammatical accuracy of translated sentences. Unlike RNN models, which tend to follow the word order of the source language, Transformers utilize cross-attention layers between the source and target languages. This allows them to arrange translated words in an order that sounds more natural in the target language, even if it differs significantly from the source structure.\n",
    "  \n",
    "* Finally, Transformers enable parallelization, which makes it feasible to train them on multiple GPUs, speeding up the training process. \n",
    "\n",
    "\n",
    "To complete this assignment, you will progress through four different stages (tasks):\n",
    "\n",
    "**Task 1. Data Preparation (5 points)**\n",
    "\n",
    "**Task 2. Model Architecture (5 points)**\n",
    "\n",
    "**Task 3. Training and Evaluation (5 points)**\n",
    "\n",
    "**Task 4. Autoregressive Translation (5 points)**\n",
    "\n",
    "### **Deliverables:** \n",
    "\n",
    "Please submit below files to Moodle:\n",
    "\n",
    "* ex5.ipynb\n",
    "* 'model.pth'\n",
    "* 'translation.npy'\n",
    "\n",
    "### **Data**\n",
    "\n",
    "The dataset used for this exercise consists of a set of French sentenceas and their equivalent English translations.\n",
    "\n",
    "*Note:* Your dataset path should point to the \"dataset_ex5\" folder, which contains two CSV files, each containing 137860 short sentences:\n",
    "\n",
    "    small_vocab_fr.csv: French sentences.\n",
    "    small_vocab_en.csv: Corresponding English translations.\n",
    "\n",
    "Be mindful of any extra folder levels that may be created when extracting the \"dataset_ex5.zip\" file.\n",
    "\n",
    "### **Useful links**\n",
    "\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "\n",
    "\n",
    "After downloading the data and setting up the folders, you are ready to begin the exercise tasks. Let's get started!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f8445f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "skip_training = False   # You can set it to True if you want to run inference on your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2644c92a-195e-42a9-b63b-0f58e1f13518",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b68dce5af274ba1b727d3e111bc7e14c",
     "grade": true,
     "grade_id": "cell-19ab751311f3561a",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4929157-7d8f-4e84-85f6-c56ea7093ae1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "188b80bd0b4c8442090aaa7246bee938",
     "grade": false,
     "grade_id": "cell-172f7e1657a03eb1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Add path to the folder containing csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959359d2-e60b-4dec-a5eb-d7dabe3f1a5f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"dataset_ex5\" # you can change the path if you want to store the dataset somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905cfe39-11bc-4c1a-9b6b-1d392a7bbd2c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63ec67478d6d210d0d333ae8d6cd039c",
     "grade": true,
     "grade_id": "cell-ca1abf7548a13d3b",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb6f08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8527fd780788625a34b40348b30de440",
     "grade": false,
     "grade_id": "cell-39478e54ddb16815",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6feccd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for all libraries\n",
    "import random\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1) \n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607dc4e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535faf5a7df324791294b2fd542d2d89",
     "grade": false,
     "grade_id": "cell-815c797e06bdf55a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Select the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54ab47d-14c2-4cd0-93a1-e4efc335c13c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b59f0542e430f521a92680557a10ca8b",
     "grade": false,
     "grade_id": "cell-c544c464735ece56",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219ed6c0-92b9-4ca5-8b9f-71995799314f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "851f4281950105edf2038c35a19db963",
     "grade": false,
     "grade_id": "cell-91357d14fc99d39d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Task 1: Data Preparation (5 Points)\n",
    "\n",
    "In this task, you will preprocess the dataset to convert it into a format suitable for input to a Transformer neural model. Each subtask focuses on a specific step in the preprocessing pipeline.\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "**Task 1.1: Tokenizaion** (1 point)\n",
    "\n",
    "**Task 1.2: Building Vocabulary** (1 point)\n",
    "\n",
    "**Task 1.3: Sentence Embedding** (1 points)\n",
    "\n",
    "**Task 1.4: Positional Encodding** (2 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9febf38-0a1a-4ff3-94a7-eb232c21eef1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f4723736e3e4512d6d08504b7a0dbd6",
     "grade": false,
     "grade_id": "cell-291860bd3393c1d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 1.1: Tokenizaion\n",
    "\n",
    "In this task, we use a basic tokenization method for simplicity and to avoid potential library mismatch issues that could arise across different students' systems and environments. While more advanced tokenization methods are available through specialized libraries, this basic approach ensures consistency and focuses on the core concept of tokenization. Our method uses Python's built-in tools and includes the following steps: \n",
    "\n",
    "1. Lowercasing: Convert all characters in the sentence to lowercase.\n",
    "2. Filtering Characters: Define a set of characters to be removed from the sentences, replacing them with an empty string.\n",
    "3. Splitting: Split the sentence into tokens based on spaces.\n",
    "   \n",
    "Run the cell below to load the data, observe basic statistics, and examine some sample sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ecf7ac-b2ce-4e9e-9560-8bc53c616282",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your data\n",
    "en_df = pd.read_csv(os.path.join(path , 'small_vocab_en.csv'), header=None, usecols=[0])\n",
    "fr_df = pd.read_csv(os.path.join(path, 'small_vocab_fr.csv'), header=None, usecols=[0])\n",
    "\n",
    "english_sentences = en_df[0].values\n",
    "french_sentences = fr_df[0].values\n",
    "\n",
    "print(f'There are {len(english_sentences)} English sentences in data')\n",
    "print(f'There are {len(french_sentences)} French sentences in data')\n",
    "print('Here are some examples:')\n",
    "e = [ 0, 1000, 3000]\n",
    "for i in e:\n",
    "    print(10*\"-\")\n",
    "    print(english_sentences[i])\n",
    "    print(french_sentences[i])\n",
    "print(100*\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30eefb6-5483-4cf5-839c-eb72aa892eb8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d56cb799102b3f767bfccf73c33b13e3",
     "grade": false,
     "grade_id": "cell-e1accedab7f1151b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the \"tokenize\" function by filling in the blanks based on the detailed guidance provided within the code comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a18098-1a7f-4758-84f4-4a7f6bfa40d3",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e136e2d6e0d0fe41dcc0ed1b4265e4b3",
     "grade": false,
     "grade_id": "cell-d953178b4a066fcc",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenize function \n",
    "def tokenize(sentences):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of sentences by:\n",
    "    1. Converting all text to lowercase.\n",
    "    2. Removing special characters listed in \"filters\".\n",
    "    Hint: you can use \"str.maketrans\" to creates a translation table to remove unwanted characters defined in \"filters\".\n",
    "    3. Splitting each sentence into a list of words.\n",
    "    \"\"\"\n",
    "    filters = '.?!#$%&()*+,-/:;<=>@«»\"\"[\\\\]^_`{|}~\\t\\n'\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return tokenized_list\n",
    "    \n",
    "# Tokenize English and French sentences\n",
    "tokenized_en = tokenize(english_sentences)\n",
    "tokenized_fr = tokenize(french_sentences)\n",
    "for i in e:\n",
    "    print(10*\"-\")\n",
    "    print(tokenized_en[i])\n",
    "    print(tokenized_fr[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aec479-83e4-447c-8a1d-3fdb7345e560",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d86e30d16b663ce919d1d9626a4fce3",
     "grade": false,
     "grade_id": "cell-68795cbd0a67b2fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to check the correctness of your solution to the tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45a918-bf00-4e55-b019-931c192d0d31",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4e28394d45a6e9c9f692050580a042f",
     "grade": true,
     "grade_id": "cell-ddea51129d5d8a84",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "\n",
    "# Test the tokenize function with example sentences\n",
    "test_sentences = [\"Hello, world!\", \"Python is fun.\", \"Let's tokenize this: right?\"]\n",
    "\n",
    "# Expected output: lowercase, special characters removed, tokenized words\n",
    "expected_output = [\n",
    "    [\"hello\", \"world\"],\n",
    "    [\"python\", \"is\", \"fun\"],\n",
    "    [\"let's\", \"tokenize\", \"this\", \"right\"]\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Run the student's tokenize function\n",
    "    tokenized_output = tokenize(test_sentences)\n",
    "    # Check if the output matches the expected output\n",
    "    assert tokenized_output == expected_output, (f\"Test failed!\\nExpected: {expected_output}\\nGot: {tokenized_output}\")\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    all_tests_successful = False\n",
    "\n",
    "if all_tests_successful:\n",
    "    print(\"Test passed: The tokenize function is working as expected.\")\n",
    "else:\n",
    "    feedback_txt.append('-1 point from the Task 1.1.Tokenizaion: The tokenize() output did not match the expected tokens in the visible test (lowercasing, removing the given special characters, and splitting correctly).')\n",
    "    grade -= 1 \n",
    "    raise AssertionError(\"Tokenization process failed & is incorrect according to the given instructions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc71a52-0148-4bce-af72-799047b0f653",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0907496be0bbf0e8a9587a38d054ac63",
     "grade": false,
     "grade_id": "cell-5c42169eb0d9ba7c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 1.2: Building Vocabulary\n",
    "\n",
    "In this step, we will convert tokenized sentences into lists of integers. This is achieved by defining a dictionary of unique words for each language and assigning a unique integer to each word.\n",
    "\n",
    "You may recall practicing a similar concept in Exercise 4, where you built a character-based dictionary. In this exercise, we are building a word-level dictionary, where each entry in the dictionary represents a unique word.\n",
    "\n",
    "In addition to the set of unique words in the dataset, the vocabulary must include three special tokens:\n",
    "\n",
    "1.  PAD: Padding Token (0)\n",
    "2.  SOS: Start of Sentence (1)\n",
    "3.  EOS: End of Sentence (2)\n",
    "\n",
    "Complete the \"build_vocab\" function by filling in the blanks according to the provided instructions in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1d488c-b866-46da-91c9-857bc7540d7b",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "54b4b871a691fb80fb59f818abe16af6",
     "grade": false,
     "grade_id": "cell-acd8f7a88e74d5a4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create vocabulary with special tokens\n",
    "def build_vocab(tokenized_sentences):\n",
    "    special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
    "    # build vocab by applying \"Counter\" for sentence in tokenized_sentences and for token in sentence\n",
    "    # add special tokens\n",
    "    # word2idx = ? (a dictionary for mapping word to index)\n",
    "    # idx2word = ? (a dictionary for index to word)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "en_word2idx, en_idx2word = build_vocab(tokenized_en)\n",
    "fr_word2idx, fr_idx2word = build_vocab(tokenized_fr)\n",
    "\n",
    "print(\"Here are some examples from our English dictionary: \")\n",
    "print(100 * \"-\")\n",
    "\n",
    "# Display first 10 words and their indices from en_word2idx\n",
    "for i, (key, value) in enumerate(en_word2idx.items()):\n",
    "    print(f'word: {key}, index: {value}')\n",
    "    if i == 9:  # After 10 iterations, break\n",
    "        break\n",
    "\n",
    "print(10 * \"_\")\n",
    "\n",
    "# Display first 10 indices and their words from en_idx2word\n",
    "for i, (key, value) in enumerate(en_idx2word.items()):\n",
    "    print(f'index: {key}, word: {value}')\n",
    "    if i == 9:  # After 10 iterations, break\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275fc4ac-53c1-4bf7-bd42-a7d601723865",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "261a033cdd5a6804343cc952a416711d",
     "grade": false,
     "grade_id": "cell-6edfda4eb54a1956",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to check the correctness of your solution to the build_vocab function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8c970-ab69-400a-aa41-44b863d5b071",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f7eb9422321ffc7216de801b27c0980",
     "grade": true,
     "grade_id": "cell-ef9dbc1c8e111c54",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests for Task 1.2 - Building Vocabulary\n",
    "all_tests_successful = True\n",
    "sample_tokenized_sentences = [[\"hello\", \"world\"], [\"hello\", \"my\", \"friend\"]]\n",
    "\n",
    "expected_special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
    "expected_vocab = expected_special_tokens + [\"friend\", \"hello\", \"my\", \"world\"]\n",
    "\n",
    "try:\n",
    "    result = build_vocab(sample_tokenized_sentences)\n",
    "    assert isinstance(result, tuple) and len(result) == 2, \"Function must return a tuple of (word2idx, idx2word)\"\n",
    "    word2idx, idx2word = result\n",
    "    assert isinstance(word2idx, dict) and isinstance(idx2word, dict), \"Both outputs must be dictionaries\"\n",
    "except Exception as e:\n",
    "    all_tests_successful = False\n",
    "    print(f\"Error during vocabulary building: {e}\")\n",
    "\n",
    "if all_tests_successful:\n",
    "    try:\n",
    "        for i, token in enumerate(expected_special_tokens):\n",
    "            assert word2idx[token] == i, f\"Special token {token} should have index {i}\"\n",
    "\n",
    "        #Checking that all unique words are present and sorted correctly\n",
    "        assert sorted(word2idx.keys()) == sorted(expected_vocab), \"Vocabulary words not sorted or missing and does not match expected words\"\n",
    "        assert len(word2idx) == len(idx2word), \"Mismatch between word2idx and idx2word lengths\"\n",
    "        assert all(idx2word[word2idx[w]] == w for w in word2idx), \"word2idx and idx2word mappings incorrect\"\n",
    "\n",
    "        assert all(isinstance(k, str) for k in word2idx.keys()), \"Vocabulary keys must be strings.\"\n",
    "\n",
    "    except AssertionError as e:\n",
    "        all_tests_successful = False\n",
    "        error_message = str(e)  # store error text\n",
    "        print(e)\n",
    "        feedback_txt.append(f\"-1 point from Task 1.2 Building Vocabulary: {error_message}\")\n",
    "\n",
    "if all_tests_successful:\n",
    "    print(\"Vocabulary test passed!\")\n",
    "else:\n",
    "    feedback_txt.append(f\"-1 point from Task 1.2 Building Vocabulary: {msg}. \"\n",
    "        f\"Expected vocabulary {expected_vocab}.\")\n",
    "    grade -= 1\n",
    "    raise AssertionError(\"Vocabulary test failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4604ca38-063e-46cb-a40e-57192653a316",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc164a3386abefc31af298f532ab98d0",
     "grade": false,
     "grade_id": "cell-2b2c9232a9907c80",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Dataset Class\n",
    "\n",
    "In this step, we will use a custom dataset class specifically designed for our translation task. You do not need to implement anything for this step, as the dataset class is already provided for you. This class incorporates essential preprocessing steps, such as padding, truncation, and the addition of special tokens.\n",
    "\n",
    "As discussed earlier, sentences in our dataset have varying lengths. To ensure all sentences are of the same length (a requirement for the Transformer model), we will define a fixed length for input sequences. The preprocessing steps performed by the dataset class include:\n",
    "\n",
    "* Special Tokens: The 'SOS' token is added at the beginning of each sentence, and the 'EOS' token is added at the end.\n",
    "* Truncation: Sentences that exceed the defined maximum length will be truncated to fit the specified length.\n",
    "* Padding: The 'PAD' token is appended to sentences shorter than the maximum length until they reach the required length.\n",
    "\n",
    "Take some time to go through the dataset class and its methods to observe how these preprocessing steps are implemented. Understanding the class structure will help you in tasks where you might need to customize or extend the dataset functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742ff0d6-758b-43c6-a870-aab4a73746dd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dataset class with padding applied in __getitem__\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_sentences, tgt_sentences, src_vocab, tgt_vocab, seq_len=30):\n",
    "        self.src_sentences = src_sentences\n",
    "        self.tgt_sentences = tgt_sentences\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_sentences)\n",
    "\n",
    "    def pad_sequence(self, tokens, vocab, is_target=False):\n",
    "        \"\"\"\n",
    "        Pads a sequence of tokens to the fixed length `seq_len`.\n",
    "        Adds <SOS> at the start, <EOS> at the end, and pads with <PAD>.\n",
    "        Trims if the sequence is longer than `seq_len`.\n",
    "        \"\"\"\n",
    "        tokens = [vocab[\"<SOS>\"]] + [vocab.get(token, vocab[\"<PAD>\"]) for token in tokens]\n",
    "        tokens.append(vocab[\"<EOS>\"])  \n",
    "        tokens = tokens[:self.seq_len]  \n",
    "        tokens += [vocab[\"<PAD>\"]] * (self.seq_len - len(tokens))  \n",
    "        return tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        src_tokens = self.src_sentences[idx]\n",
    "        tgt_tokens = self.tgt_sentences[idx]\n",
    "        \n",
    "        # Apply padding to both the source and target sentences\n",
    "        src_padded = self.pad_sequence(src_tokens, self.src_vocab, is_target=False)\n",
    "        tgt_padded = self.pad_sequence(tgt_tokens, self.tgt_vocab, is_target=True)\n",
    "        \n",
    "        # Convert to tensors and move to device (GPU or CPU)\n",
    "        src_item = torch.tensor(src_padded).to(device)\n",
    "        tgt_item = torch.tensor(tgt_padded).to(device)\n",
    "    \n",
    "        return src_item, tgt_item\n",
    "\n",
    "# Instantiate and test the dataset, let the French be as source language and English as target language.\n",
    "dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Test the DataLoader\n",
    "for src_batch, tgt_batch in dataloader:\n",
    "    print(\"Source batch:\", src_batch)\n",
    "    print(src_batch.size())\n",
    "    print(10*\"_\")\n",
    "    print(\"Target batch:\", tgt_batch)\n",
    "    print(tgt_batch.size())\n",
    "    print(10*\"_\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cbf6fe-050c-4357-ae8f-112a4021b231",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a920ba916dac6fa8032a24c0603867eb",
     "grade": false,
     "grade_id": "cell-5ddb30e4fc5234fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 1.3: Sentence Embedding\n",
    "\n",
    "Words, by themselves, are discrete symbols that neural networks cannot process directly. To make them understandable to the model, we use **embedding layers**. These layers transform words or tokens into dense, fixed-size vectors, where each word is represented by a unique vector. The embedding layer maps words into a continuous vector space, enabling semantically similar words to be closer to each other in this space. This approach is far more compact and efficient than sparse, high-dimensional representations like one-hot encoding.\n",
    "\n",
    "Embedding layers are particularly useful in natural language processing tasks such as **machine translation**, where words in different languages must be represented in a way that enables the model to learn their relationships.\n",
    "\n",
    "In this step, you are asked to define separate PyTorch embedding layers for both the source and target languages, and then pass the src_batch and tgt_batch through these layers. In the cell below, complete the code by filling in the blanks according to the provided instructions. After running the cell, pay attention to the input and output dimensions of the embedding layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7f794-94bc-4332-a186-7c0d11fc218f",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e641ffdf106f543f00d6a776e3963eb",
     "grade": false,
     "grade_id": "cell-fab72a3e23c8b68e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "vsize_src = len(fr_word2idx)\n",
    "vsize_tgt = len(en_word2idx)\n",
    "\n",
    "# data: let the French be as source language and English as target language.\n",
    "dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Use 'next' to get a batch from the DataLoader iterator\n",
    "src_batch, tgt_batch = next(iter(dataloader))\n",
    "\n",
    "# embedding_fr = ? (define an embedding layer for French words in your French Vocabulary)\n",
    "# embedding_fr.to(device)\n",
    "# output_embedding_fr = ? (pass src_batch through embedding_fr)\n",
    "\n",
    "# embedding_en = ? (define an embedding layer for English words in your English Vocabulary)\n",
    "# embedding_en.to(device)\n",
    "# output_embedding_en = ? \n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "print(10*\"_\")\n",
    "print(src_batch.size())\n",
    "print(output_embedding_fr.size())\n",
    "print(10*\"_\")\n",
    "print(tgt_batch.size())\n",
    "print(output_embedding_en.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc911b0-97c1-4924-817f-417696c65d44",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b5ad5665bb0536a3e107f8d3e8668419",
     "grade": false,
     "grade_id": "cell-db2fee17b9a7a42a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below to check the correctness of your solution for the sentence embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30669a20-b2f9-4e86-96ac-15af4cdb01b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b5818ab1cdb666841ab2a892089ff20",
     "grade": true,
     "grade_id": "cell-3e23c383eb021716",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "\n",
    "try:\n",
    "    # Check if the embedding for French has the correct shape\n",
    "    assert output_embedding_fr.shape == (src_batch.size(0), src_batch.size(1), embedding_size), (\n",
    "        \"Embedding for French is incorrect! \"\n",
    "        f\"Expected: {(src_batch.size(0), src_batch.size(1), embedding_size)}, \"\n",
    "        f\"Got: {output_embedding_fr.shape}\"\n",
    "    )\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "try:\n",
    "    # Check if the embedding for English has the correct shape\n",
    "    assert output_embedding_en.shape == (tgt_batch.size(0), tgt_batch.size(1), embedding_size), (\n",
    "        \"Embedding for English is incorrect! \"\n",
    "        f\"Expected: {(tgt_batch.size(0), tgt_batch.size(1), embedding_size)}, \"\n",
    "        f\"Got: {output_embedding_en.shape}\"\n",
    "    )\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "if not all_tests_successful:\n",
    "    feedback_txt.append(\n",
    "        f\"-1 point from the Task 1.3.Sentence Embedding: \"\n",
    "        f\"Expected French embedding shape {(src_batch.size(0), src_batch.size(1), embedding_size)}, \"\n",
    "        f\"English embedding shape {(tgt_batch.size(0), tgt_batch.size(1), embedding_size)}, \"\n",
    "        f\"but got French {output_embedding_fr.shape} and English {output_embedding_en.shape}.\"\n",
    "    )\n",
    "    raise AssertionError(\n",
    "        f\"Embedding shapes do not match expected values. \"\n",
    "        f\"French embedding: {output_embedding_fr.shape}, English embedding: {output_embedding_en.shape}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede8445b-610d-4f87-8100-866155af1a23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "80fe9e22decdc772b4a34aaf55fa90f7",
     "grade": true,
     "grade_id": "cell-357cb8363da54e4e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8ae0f-ece1-4596-8037-89631b8d2075",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "478bcf1a7168a93463c0482b18b80bbf",
     "grade": false,
     "grade_id": "cell-8372a80b53e83a10",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 1.4: Positional encoding\n",
    "\n",
    "In sequence models like the Transformer, the model needs a way to understand the relative positions of words in a sequence. Since the Transformer model does not inherently process sequential data in a time-dependent manner (unlike RNNs or LSTMs), we need to explicitly provide information about the position of each word in the input sequence.\n",
    "\n",
    "The Positional Encoding layer is used to add this positional information to the word embeddings. It generates a vector for each position in the sequence and combines it with the word embedding to provide both the content and position information. The positional encoding is computed using sine and cosine functions of different wavelengths, which allows the model to easily learn relative positions.\n",
    "\n",
    "In this step, your task is to implement the Positional Encoding layer. You should:\n",
    "\n",
    "1. Compute the positional encodings using sine and cosine functions.\n",
    "2. Register the positional encodings as a buffer so they are not considered trainable parameters.\n",
    "3. Add the positional encoding to the word embeddings during the forward pass.\n",
    "\n",
    "Once you have implemented the layer, you can test if it works correctly by running the test cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfdac1-3bd3-4a57-b034-4aeb97cf1b9d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66ff2d4d083b95d6edb7f6e17233b63e",
     "grade": false,
     "grade_id": "cell-de1d08bfdff6d81e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        \n",
    "        # Initialize a tensor to store positional encodings for each position up to max_len\n",
    "        pos_encoding = torch.zeros(max_len, embed_size)\n",
    "        # Create a tensor for positions, where each position corresponds to a word's position in the sequence\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # 1. Create a tensor `div_term` to scale the positional encoding values. \n",
    "        \n",
    "        # Hint:\n",
    "        # This is based on the formula for positional encoding where each dimension has a different frequency.\n",
    "        # We generate a range of values from 0 to embed_size, stepping by 2 (for even indices), and multiply it by a scaling factor.\n",
    "        # The scaling factor (-math.log(10000.0) / embed_size) ensures the frequencies decay logarithmically.\n",
    "        \n",
    "        # 2. Apply the sine function to the even indices of the positional encoding matrix.\n",
    "        # 3. Apply the cosine function to the odd indices of the positional encoding matrix.\n",
    "        \n",
    "        # Hint:\n",
    "        # The `position` tensor holds the position values for each token, and `div_term` scales those values.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # Register as buffer so it is not considered as a parameter during training\n",
    "        self.register_buffer('pos_encoding', pos_encoding.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add positional encoding to embeddings\n",
    "        x = x * math.sqrt(self.embed_size)\n",
    "        x = x + self.pos_encoding[:, :x.size(1), :].to(x.device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a3335e-1dff-4fe8-aebd-8367d2359da5",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%% Applying positional encoding \n",
    "positional_encoding = PositionalEncoding(embedding_size, 512)\n",
    "output_pe_fr = positional_encoding (output_embedding_fr)\n",
    "output_pe_en = positional_encoding (output_embedding_en)\n",
    "print(10*\"_\")\n",
    "print(output_pe_fr.size())\n",
    "print(output_pe_en.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa00a4a-cbc5-4ef4-873f-b4d0dafa33ea",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c984245772da26b8514ebcd67d96da8d",
     "grade": true,
     "grade_id": "cell-24f5a81611dd6e96",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "# Test if positional encoding has been implemented correctly\n",
    "\n",
    "try:\n",
    "    # Initialize the positional encoding with a fixed embed size and max_len\n",
    "    test_pos_enc = PositionalEncoding(embed_size=128, max_len=512)\n",
    "    \n",
    "    # Let's assume the input sequence is a batch of embeddings (value doesn't matter for this test)\n",
    "    src_batch = torch.zeros(1, 10, 128)  # batch_size=1, seq_len=10, embed_size=128\n",
    "    tgt_batch = torch.zeros(1, 10, 128)\n",
    "    \n",
    "    # Pass through the positional encoding layer\n",
    "    encoded_src_batch = test_pos_enc(src_batch)\n",
    "    encoded_tgt_batch = test_pos_enc(tgt_batch)\n",
    "    \n",
    "except Exception as e:\n",
    "    all_tests_successful = False\n",
    "    print(f\"Error during building PositionalEncoding: {e}\")\n",
    "    \n",
    "\n",
    "\n",
    "try:\n",
    "    # Check the output dimensions\n",
    "    assert encoded_src_batch.size() == src_batch.size(), (\n",
    "        f\"Expected {src_batch.size()}, but got {encoded_src_batch.size()}\"\n",
    "    )\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "\n",
    "try:\n",
    "    assert encoded_tgt_batch.size() == tgt_batch.size(), (\n",
    "        f\"Expected {tgt_batch.size()}, but got {encoded_tgt_batch.size()}\"\n",
    "    )\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "\n",
    "try:\n",
    "    # Ensure that adding positional encoding to embeddings changes the values\n",
    "    # The output at the first position should be different than at other positions\n",
    "    assert not torch.allclose(encoded_src_batch[:, 0, :], encoded_src_batch[:, 1, :], atol=1e-5), \\\n",
    "        \"Positional encoding should differentiate between different positions in the sequence.\"\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "\n",
    "if all_tests_successful:\n",
    "    print(\"Positional Encoding has been implemented correctly!\")\n",
    "else:\n",
    "    feedback_txt.append(\n",
    "        '-2 points from Task 1.4.Positional Encoding: The positional encoding must output the same shape as the input embeddings '\n",
    "        'and differentiate positions.'\n",
    "        'Expected shape: {src_batch.size()}, got: {encoded_src_batch.size()} or positional differentiation might be missing.'\n",
    "    )\n",
    "    grade -= 2\n",
    "    raise AssertionError(\n",
    "        f\"Hidden test failed for positional encoding. Expected shape: {src_batch.size()}, \"\n",
    "        f\"got: {encoded_src_batch.size()}. Check that positional encoding is applied correctly to all positions.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baf7c61-c078-45c3-977e-fbefe51e3beb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29456a0467e8a115f33b88974dd4d293",
     "grade": false,
     "grade_id": "cell-093e16e5c4566696",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Model Architecture (5 points)\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "**Task 2.1: Designing a basic transformer block** (3 points)\n",
    "\n",
    "**Task 2.2: Adding Encoder and Decoder blocks** (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1ff3a1-5f60-4baa-9eb9-17d899db9df5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b95686179c6ec14218111c952981a8a7",
     "grade": false,
     "grade_id": "cell-791a329ab0d9161b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 2.1: Designing a basic transformer block\n",
    "\n",
    "In this task, you will implement a simple Transformer model. This model will take source and target sequences as input, apply embeddings and positional encodings, pass the result through a Transformer block, and finally project the output to the target vocabulary space. Before passing the input through the Transformer block, you need to compute two types of masks:\n",
    "\n",
    "**Padding Mask:** This mask is applied to the source sequence and to the target sequence to prevent the model from attending to padding tokens, which should be ignored during training. The padding mask is implemented in the create_pad_mask method.\n",
    "\n",
    "**Target Mask (tgt_mask):** This mask is used to prevent the model from using future target steps to predict the current output. If the model could access future information, it would already know the solution, making training redundant. The tgt_mask helps ensure causal attention by masking out future tokens in the target sequence.\n",
    "\n",
    "These two masks are essential for enabling effective training and maintaining the correct flow of information through the model. \n",
    "\n",
    "For this task, you can use the pre-implemented embedding and positional encoding blocks. In the MySimpleTransformer template provided below, fill in the blanks as instructed in the code. Once you have implemented the MySimpleTransformer class, you can test the correctness of your solution by running the test cell. \n",
    "\n",
    "You may receive a warning about using 'batch_first' during the initialization of the Transformer block. Please ignore it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f4ccdb-b179-4d23-952d-bdc91600d5b6",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7ff5491cc33c712ba4fdb589c0db60e",
     "grade": false,
     "grade_id": "cell-2c2066cf6ea79138",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MySimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, embed_size, num_heads, hidden_dim, num_encoder_layers, num_decoder_layers, max_len=512):\n",
    "        super(MySimpleTransformer, self).__init__()\n",
    "        # Initialize layers as below:\n",
    "        # Embedding layer for source language tokens\n",
    "        # embedding layer for target language tokens\n",
    "        # Positional encoding\n",
    "        # Transformer block (Hint: use nn.Transformer)\n",
    "        # Final linear layer to project transformer output to vocab size (Hint: use nn.Linear)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None, tgt_mask=None):\n",
    "        \n",
    "        # 1. Get embeddings for source and target.\n",
    "        # 2. apply positional encoding to embedded source and target\n",
    "        # 3. Create a causal mask for the target to prevent seeing future tokens\n",
    "        # 4. Forward pass to Transformer block with masking\n",
    "        # 5. Project to vocabulary size\n",
    "            \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def get_tgt_mask(self, tgt):\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = self.transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        return tgt_mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix):\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        pad_token = 0\n",
    "        return (matrix == pad_token)\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f4760-5840-49c9-b60e-1724a6a9d99c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb56f4b0bc7c6318b4f422e8c35a8499",
     "grade": true,
     "grade_id": "cell-e15a20fbf192c2b8",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "\n",
    "try:\n",
    "    embedding_size = 512\n",
    "    vsize_src = len(fr_word2idx) # 336 \n",
    "    vsize_tgt = len(en_word2idx) # 201\n",
    "    hdim = 128\n",
    "    model = MySimpleTransformer(vsize_src, vsize_tgt, embedding_size, 2, hdim, 3, 3, max_len=512)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Data\n",
    "    dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    src_batch, tgt_batch = next(iter(dataloader))\n",
    "        \n",
    "    tgt_mask = model.get_tgt_mask(tgt_batch)\n",
    "    src_padding_mask = model.create_pad_mask(src_batch)\n",
    "    tgt_padding_mask = model.create_pad_mask(tgt_batch)\n",
    "    output = model(src_batch, tgt_batch, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_mask = tgt_mask )\n",
    "except Exception as e:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.1: Visible test: Error during building MySimpleTransformer: {e}\")\n",
    "    print(f\"Error during building MySimpleTransformer: {e}\")\n",
    "    \n",
    "\n",
    "try:\n",
    "    # Check if the output shape is correct: [batch_size, seq_len, vocab_size_tgt]\n",
    "    if output.size() != (1, 10, vsize_tgt):\n",
    "        raise AssertionError(\n",
    "            f\"Expected output shape (1, 10, {vsize_tgt}), but got {output.size()}\"\n",
    "        )\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"- Error in Task 2.1: Visible test:  {e}\")\n",
    "    print(e)\n",
    "    \n",
    "\n",
    "try:\n",
    "    # Check if the number of trainable parameters is correct\n",
    "    num_params = get_num_trainable_parameters(model)\n",
    "    expected_num_parameters = 10641353\n",
    "    if num_params != expected_num_parameters:\n",
    "        raise AssertionError(\n",
    "            f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\"\n",
    "        )\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"- Error in Task 2.1: Visible test: {e}\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "pos_encoding_found = any(\n",
    "    isinstance(layer, PositionalEncoding) \n",
    "    for name, layer in model.named_modules()\n",
    ")\n",
    "try:\n",
    "    assert pos_encoding_found, \"No PositionalEncoding layer found in your transformer model.\"\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append('-1 points from Task 2.1: Missing positional encoding in the transformer model.')\n",
    "    raise AssertionError(e)\n",
    "\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")\n",
    "else:\n",
    "    feedback_txt.append('-3 points from the Task 2.1: Designing a basic transformer block. ')\n",
    "    grade -= 3\n",
    "    raise AssertionError(\"Viisble tests in Task 2.1 failed. Check feedback for details.\")\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa97de74-2b5a-4521-9474-04f9b7112e0b",
   "metadata": {
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(src_batch.size())\n",
    "print(tgt_batch.size())\n",
    "\n",
    "print(src_padding_mask.size())\n",
    "print(src_batch.cpu().detach().numpy())\n",
    "print(src_padding_mask.cpu().detach().numpy())\n",
    "\n",
    "print(tgt_mask.size())\n",
    "print(tgt_mask.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce03f944-f320-4ab2-ba28-cbeb8cb7927a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ffa9c18d5dc8110338e939c9d337b22",
     "grade": false,
     "grade_id": "cell-c94bd78980ab8c0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Task 2.2: Adding Encoder and Decoder blocks\n",
    "\n",
    "The MySimpleTransformer class you implemented uses the Transformer module of PyTorch, which consists of two main parts: the encoder and the decoder. Each part of the model contains several layers of self-attention and feedforward neural networks, with the encoder and decoder connected by a cross-attention mechanism.\n",
    "\n",
    "**Self-Attention:** In the context of machine translation, self-attention allows each word in a sequence (either in the source or target language) to attend to every other word in the same sequence, regardless of their position. This mechanism enables the model to capture long-range dependencies and relationships within the sentence.\n",
    "\n",
    "**Cross-Attention:** This occurs in the decoder block, where the model attends to the encoder's output. In machine translation, cross-attention allows the decoder to focus on relevant parts of the input sequence (source language) when generating the output sequence (target language). It essentially \"crosses\" between the encoder and decoder, enabling the model to translate based on the context of both source and target sentences.\n",
    "\n",
    "Next, we will modify our Transformer block by manually separating the encoder and decoder components. This separation is necessary because, during inference (translation), the source sentence must pass through the encoder, and the generated target sentence must pass through the decoder one token at a time. This step is crucial since the translation process is autoregressive, meaning each word is predicted based on the previously generated words.\n",
    "\n",
    "In the cell below, complete the MyTransformer class as an updated version of our model class. This updated version will be used to train our neural machine translation system. Ensure that the encoder and decoder components are implemented separately, as this is important for managing the inference process later.\n",
    "\n",
    "Hints:\n",
    "\n",
    "1. The updated model is similar to the simple model from Task 2.1, except that the encoder and decoder are separated to allow individual calls. Remember to include the source and target padding masks, as well as the target causal mask (tgt_mask).\n",
    "\n",
    "2. The layer initialization for the updated model should be identical to the simple model. Specifically, the *init* method of the MyTransformer class should mirror the MySimpleTransformer class.\n",
    "\n",
    "3. Use nn.Transformer.encoder(src_embedded, src_key_padding_mask=src_padding_mask)\n",
    "\n",
    "4. Use nn.Transformer.decoder(tgt_embedded, memory, tgt_mask=tgt_mask, tgt_key_padding_mask=tgt_padding_mask)\n",
    "\n",
    "Once you have implemented the MyTransformer class, you can test the correctness of your solution by running the test cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05c4ec1-3f32-4d12-b0e2-a2bd1643b0de",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3e19cf10486e97e5f1e11d783d36ea26",
     "grade": false,
     "grade_id": "cell-8acf47d49c2ce752",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MyTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, embed_size, num_heads, hidden_dim, num_encoder_layers, num_decoder_layers, max_len=512):\n",
    "        super(MyTransformer, self).__init__()\n",
    "        # Initialize layers similar to MySimpleTransformer\n",
    "        # Two Embedding layers for source and target text\n",
    "        # Positional encoding\n",
    "        # Transformer block\n",
    "        # Final linear layer to project transformer output to vocab size\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def encode(self, src, src_padding_mask):\n",
    "        # 1. Get embeddings for source \n",
    "        # 2. apply positional encoding to embedded source \n",
    "        # 3. Forward pass to Transformer encoder block with src_key_padding_mask\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return encoded\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask, tgt_padding_mask):\n",
    "        # 1. Get embeddings for target\n",
    "        # 2. apply positional encoding to embedded target\n",
    "        # 3. Forward pass target and memory (output of encode) to Transformer decoder block with tgt_mask\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "    def forward(self, src, tgt, src_padding_mask=None, tgt_padding_mask=None, tgt_mask=None):\n",
    "        # 1. pass source through encode block (name it as memory)\n",
    "        # 2. pass target and memory through decode block\n",
    "        # 3. Project to vocabulary size  \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        return  output_decoder, output \n",
    "    \n",
    "    def get_tgt_mask(self, tgt):\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(tgt.device)\n",
    "        return tgt_mask\n",
    "    \n",
    "    def create_pad_mask(self, matrix):\n",
    "        # If matrix = [1,2,3,0,0,0] where pad_token=0, the result mask is\n",
    "        # [False, False, False, True, True, True]\n",
    "        pad_token = 0\n",
    "        return (matrix == pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d421762-6a59-4a20-8c05-594ea71eafac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f90128911c29f354075e3e2330684e26",
     "grade": true,
     "grade_id": "cell-71426ea9698049d6",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "\n",
    "try:\n",
    "    embedding_size = 512\n",
    "    vsize_src = len(fr_word2idx) # 336 \n",
    "    vsize_tgt = len(en_word2idx) # 201\n",
    "    hdim = 128\n",
    "    model = MyTransformer(vsize_src, vsize_tgt, embedding_size, 2, hdim, 3, 3, max_len=512)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Data\n",
    "    dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=10)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    src_batch, tgt_batch = next(iter(dataloader))\n",
    "        \n",
    "    tgt_mask = model.get_tgt_mask(tgt_batch)\n",
    "    src_padding_mask = model.create_pad_mask(src_batch)\n",
    "    tgt_padding_mask = model.create_pad_mask(tgt_batch)\n",
    "    _, output = model(src_batch, tgt_batch, src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask, tgt_mask = tgt_mask )\n",
    "except Exception as e:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.2: Visible test: Error during building MyTransformer: {e}\")\n",
    "    print(f\"Error during building MyTransformer: {e}\")\n",
    "    \n",
    "\n",
    "try:\n",
    "    expected_shape = (1, 10, vsize_tgt)\n",
    "    if output.size() != expected_shape:\n",
    "        raise AssertionError(\n",
    "            f\"Expected output shape {expected_shape}, but got {output.size()}\"\n",
    "        )\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"- Error in Task 2.2: Visible test: {e}\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "try:\n",
    "    num_params = get_num_trainable_parameters(model)\n",
    "    expected_num_parameters = 10641353\n",
    "\n",
    "    if num_params != expected_num_parameters:\n",
    "        raise AssertionError(\n",
    "            f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\"\n",
    "        )\n",
    "\n",
    "except AssertionError as e:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"- Error in Task 2.2: Visible test: {e}\")\n",
    "    print(e)\n",
    "\n",
    "\n",
    "\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")\n",
    "else:\n",
    "    feedback_txt.append('-2 points from the Task 2.2: Adding Encoder and Decoder blocks.')\n",
    "    grade -= 2\n",
    "    raise AssertionError(\"Visible tests failed. Find the details in feedback.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f478708b-68d0-4690-8bed-3629ea2e4b2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa833b18ff0a817c3b7a1c03133e50b0",
     "grade": false,
     "grade_id": "cell-2f77cd749e40453a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Task 3: Training and Validation (5 points)\n",
    "\n",
    "So far, we have defined our dataset class and the Transformer model. The next step is to train and validate the model. We will split the data into training and validation sets, with an 80% training and 20% validation ratio, and run the training and validation loops accordingly.\n",
    "\n",
    "For the model, we will define a simple Transformer with a hidden dimension of 512, 4 encoder layers, 4 decoder layers, and 6 attention heads. We will use cross-entropy loss, which is well-suited for Transformer-based machine translation because it measures the difference between the predicted probability distribution and the true distribution for each token in the sequence. This loss function helps the model optimize the prediction accuracy for each word in the target sequence. Additionally, we will use the Adam optimizer to efficiently minimize the loss. \n",
    "\n",
    "First, run the two cells below to define the data and model with the specified hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93453c6-4922-4ddf-b8dc-19558256fceb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3ac1b60f442e9b10eccc70525247757",
     "grade": false,
     "grade_id": "cell-39da44537b108c0a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Data\n",
    "\n",
    "bs = 256\n",
    "dataset = TranslationDataset(tokenized_fr, tokenized_en, fr_word2idx, en_word2idx,  seq_len=11)\n",
    "number_of_sentences = len(tokenized_fr)\n",
    "train_size = int((0.8)*number_of_sentences)\n",
    "test_size = int(number_of_sentences - train_size)\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, drop_last=True)\n",
    "print(f'Training samples: {len(train_dataset)}, Validation samples: {len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd32c9e-9868-4f8a-819d-6816332a9ea7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ada45fd309f153dcb7c3f001c5acddc8",
     "grade": false,
     "grade_id": "cell-47986e3c9b4441ee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "embedding_size = 240 # embed_dim must be divisible by num_heads\n",
    "vsize_src = len(fr_word2idx) # 336 \n",
    "vsize_tgt = len(en_word2idx) # 201\n",
    "hdim = 512\n",
    "model = MyTransformer(vsize_src, vsize_tgt, embedding_size, 6, hdim, 4, 4, max_len=256)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a4a14-51e3-42d2-babd-2fd67708cac0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e8a960216e1f9167a4a54e07c15b093",
     "grade": false,
     "grade_id": "cell-e4d856271fb971bb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In the cell below, complete the training and validation loops by filling in the blanks as instructed in the code. Once you have implemented the loops, run the cell to train the model for 10 epochs. \n",
    "\n",
    "If you have limited computational resources, you may train for fewer epochs until the validation loss reaches below 0.1 (This can be reached after about 3 epochs). However, keep in mind that the performance of your model will be very poor on the translation task if it is not trained for enough epochs.\n",
    "\n",
    "Once you have completed the training, run the test cell to check if the training and validation loss have decreased to below 0.1. Remember to submit the trained model **model.pth** to Moodle along with your other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e63f9b-07b4-4d01-a62d-25b9deafaf5a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b079c58218c69b7d28877a40c7291ae",
     "grade": false,
     "grade_id": "cell-3e78e5aa606c25be",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "num_epochs =3\n",
    "\n",
    "if not skip_training:\n",
    "    epoch_train_losses = []\n",
    "    epoch_validation_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model = model.to(device)\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        num_samples = 0\n",
    "        for src_batch, tgt_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            # set input source as src_batch\n",
    "            # set input target as start of target sentence untill -1 (i.e. discard the last token) (past) [name it as tgt_input]\n",
    "            # set expexted target as target sentence from 1 to the end (i.e. discard the first token) (future) [name it as tgt_expected]\n",
    "            # use input source and input target (tgt_input) as input to the model\n",
    "            # expexted target will be used in loss to compare it with the model output.\n",
    "            # Remember to use padding and target causal masks on the model call:\n",
    "            # get padding mask of source,\n",
    "            # get padding mask of input target, convert it to float (.float()),\n",
    "            # get tgt_mask of input target,\n",
    "            # pass inout source, input target, source padding mask, and tgt_mask to the model to get the predictions (output).\n",
    "            \n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            \n",
    "            output = output.to(device)\n",
    "            output = output.contiguous().view(-1, vsize_tgt)\n",
    "            tgt_expected = tgt_expected.contiguous().view(-1)    \n",
    "            \n",
    "            loss = criterion(output, tgt_expected)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_samples += src_batch.shape[0]\n",
    "    \n",
    "        epoch_train_loss = total_loss / len(train_loader)\n",
    "        epoch_train_loss = round(epoch_train_loss, 4)\n",
    "        epoch_train_losses.append(epoch_train_loss)\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_train_loss}\")\n",
    "        \n",
    "        ################################################################\n",
    "        \n",
    "        model.eval()\n",
    "        validation_loss = 0\n",
    "        num_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for src_batch, tgt_batch in val_loader:\n",
    "                # set input source as src_batch\n",
    "                # set input target as start of target sentence untill -1 (i.e. discard the last token) (past) [name it as tgt_input]\n",
    "                # set expexted target as target sentence from 1 to the end (i.e. discard the first token) (future) [name it as tgt_expected]\n",
    "                # use input source and input target (tgt_input) as input to the model\n",
    "                # expexted target will be used in loss to compare it with the model output.\n",
    "                # Remember to use padding and target causal masks on the model call:\n",
    "                # get padding mask of source,\n",
    "                # get padding mask of input target, convert it to float (.float()),\n",
    "                # get tgt_mask of input target,\n",
    "                # pass inout source, input target, source padding mask, and tgt_mask to the model to get the predictions (output).\n",
    "                    \n",
    "                # YOUR CODE HERE\n",
    "                raise NotImplementedError()\n",
    "                \n",
    "                output = output.to(device)\n",
    "                output = output.contiguous().view(-1, vsize_tgt)\n",
    "                tgt_expected = tgt_expected.contiguous().view(-1)  \n",
    "                loss = criterion(output, tgt_expected)\n",
    "                validation_loss += loss.item()\n",
    "                num_samples += src_batch.shape[0]\n",
    "                \n",
    "            epoch_validation_loss = validation_loss / len(val_loader)\n",
    "            epoch_validation_loss = round(epoch_validation_loss, 4)\n",
    "            epoch_validation_losses.append(epoch_validation_loss)\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {epoch_validation_loss}\")\n",
    "        torch.save(model.state_dict(), 'model.pth')   \n",
    "            \n",
    "    print(\"Training completed.\")\n",
    "    torch.save(model.state_dict(), 'model.pth')        \n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "    plt.plot(epochs, epoch_train_losses, label='Train Loss', color='blue', marker='o')\n",
    "    plt.plot(epochs, epoch_validation_losses, label='Validation Loss', color='red', marker='x')\n",
    "    plt.title('Train vs Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d08e0-eef4-46cb-9947-2fc04e812c2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4b38225afb092c7467ab92e13c131803",
     "grade": false,
     "grade_id": "cell-890178299bdab6c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Visible test\n",
    "all_tests_successful = True\n",
    "# Tests for checking the performance of the trained model\n",
    "# Test if the train and validation losses are within the correct range\n",
    "if not skip_training:\n",
    "    try:\n",
    "        if not (epoch_train_loss <= 0.1):\n",
    "            raise AssertionError(\n",
    "                f\"Task:3: Visible test: Training loss {epoch_train_loss} must be smaller than 0.15\"\n",
    "            )\n",
    "    except AssertionError as e:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(str(e))  \n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if not (epoch_validation_loss <= 0.1):\n",
    "            raise AssertionError(\n",
    "                f\"Task:3: Visible test: Validation loss {epoch_validation_loss} must be smaller than 0.1\"\n",
    "            )\n",
    "    except AssertionError as e:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(str(e))  \n",
    "        print(e)\n",
    "\n",
    "    # Final message\n",
    "    if all_tests_successful:\n",
    "        success_str = \"Good job! The final train loss and validation loss are in the expected range!\"\n",
    "        print(f\"\\033[92m{success_str}\\033[0m\")\n",
    "    else:\n",
    "        raise AssertionError(\"Training/validation performance check failed.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72146f6b-9f87-4d59-9833-d62c12257eba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f50a38fd0bad2c9220ee97afcb6e0b4",
     "grade": true,
     "grade_id": "cell-1ac2935596c4dc68",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d97d50-f344-4bd0-9933-31e0967db8ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3cd6f4b79c9eb03096892f79e8f40433",
     "grade": false,
     "grade_id": "cell-c54877793b096ca1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Task 4: Autoregressive Translation (5 points)\n",
    "\n",
    "Finally, we are going to use the trained model to perform an actual translation task, translating French sentences into English. Exciting!\n",
    "\n",
    "The inference works in an autoregressive manner. This means that, during the translation process, the model generates each token in the target sequence one at a time, using the previously generated token as input for predicting the next one. At each step, the model uses the encoded source sentence along with the target sequence generated so far to predict the next word. This approach allows the model to produce the translation step by step, instead of generating the entire sequence at once.\n",
    "\n",
    "The steps for translation are as follows:\n",
    "\n",
    "**Source Sentence Encoding:** First, we obtain the source sequence embedding and pass it through the encoder to obtain its encoded representation.\n",
    "\n",
    "**Initializing Target Sentence with <SOS> Token:** We initialize the target sentence with the special <SOS> (Start Of Sentence) token, which indicates the beginning of the translation.\n",
    "\n",
    "**Autoregressive Loop to Translate Target Tokens One at a Time:** We enter a loop where the model predicts the next token in the sequence based on the previously generated token and the encoded source sentence. This loop continues until the model predicts the <EOS> (End Of Sentence) token or the maximum sequence length is reached.\n",
    "\n",
    "In the cell below, fill in the blanks as instructed to create the translation loop for the provided example sentences. Once you have completed the template, run it and observe the printed translation results. \n",
    "\n",
    "Remeber to submit **'translation.npy'** to Moodle along with your other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b511cb3-8a3d-4d83-b403-ffb9f4b65f5e",
   "metadata": {
    "deletable": false,
    "editable": true,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a7d107bba2e8b29eabdd5bde898afb0b",
     "grade": false,
     "grade_id": "cell-4dae7ac8dd1330c2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_len=10\n",
    "start_token=1\n",
    "end_token=2\n",
    "model.eval()  \n",
    "\n",
    "# Convert src_sentence to tokenized integers in the vocabulary dictionary \n",
    "example_source_sentences = [\"new jersey est parfois calme pendant l' automne.\", \"california est généralement calme en mars.\"]\n",
    "example_tokenized = tokenize(example_source_sentences)\n",
    "src_sentences = []\n",
    "for ex in example_tokenized:\n",
    "    ex_inds = []\n",
    "    for t in ex:\n",
    "        t_ind = fr_word2idx [t]\n",
    "        ex_inds.append(t_ind)\n",
    "    src_sentences.append(ex_inds)    \n",
    "\n",
    "translated_sequences = []\n",
    "for counter, src_sentence in enumerate(src_sentences):    \n",
    "    # Convert source tokens to Tensor \n",
    "    src_tensor = torch.tensor(src_sentence, dtype=torch.long).unsqueeze(0).to(device)  # Shape: (1, src_seq_length)\n",
    "    \n",
    "    # 1. create src_padding_mask\n",
    "    # 2. get \"memory\" by passing source with create src_padding_mask through encode block (model.encode)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    # initialize the predicted tgt_tokens (translation) with start token\n",
    "    tgt_tokens = torch.ones(1, 1).fill_(start_token).type(torch.long).to(device) #(1,1)\n",
    "\n",
    "    for i in range(seq_len-1):\n",
    "        # 1. Mask out the unpredicted tokens in the target (i.e., get tgt_mask)\n",
    "        # 2. get output by passing target (the generated part up to current `i`) and memory to decode block (model.decode)\n",
    "        # 3. remember to pass also tgt_mask\n",
    "        # 4. get a probability vector by passing output through linear layer (projection to vocabulary size)\n",
    "        # 5. use \"torch.max\" to get the index of the predicted word\n",
    "        # 6. Convert it to a tensor on device (name it as \"next_tgt_item\")\n",
    "        # 7. add \"next_tgt_item\" to tgt_tokens (use torch.cat)\n",
    "        # 8. Stop (break) if \"end_token\" is generated\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    translated_tokens = tgt_tokens.squeeze().tolist()\n",
    "    translated_sentence = ' '.join ([en_idx2word[i] for i in translated_tokens[1:]])\n",
    "    translated_sequences.append(translated_tokens)\n",
    "    print(\"original_sentence:\", example_source_sentences[counter])\n",
    "    print(\"translated_sentence:\", translated_sentence)\n",
    "    print(10*'-')\n",
    "\n",
    "np.save('translation.npy', np.array(translated_sequences, dtype=object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce7f314-8db1-408a-bf5f-e91daf531029",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "922697a48c9fcbd294bf44dd665bf0d0",
     "grade": true,
     "grade_id": "cell-afe70a33546d08c5",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaa2e51-3a7a-464a-8198-3664a3b3503b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8f309b29a656da9492ffbdabb8c83aec",
     "grade": true,
     "grade_id": "cell-17c102896a610808",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
