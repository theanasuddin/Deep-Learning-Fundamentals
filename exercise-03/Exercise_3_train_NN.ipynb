{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c92360",
   "metadata": {
    "deletable": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Important! Please do not remove any cells, including the test cells, even if they appear empty. They contain hidden tests, and deleting them could result in a loss of points, as the exercises are graded automatically. Only edit the cells where you are instructed to write your solution. Please read all the instructions in each notebook carefully.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f59ace-e393-4b71-9289-d40d3997530a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d94c2340653a0870a8d5f6ec944bc204",
     "grade": false,
     "grade_id": "cell-5ed66fa5f1209cda",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Exercise 3: Deep Learning Practices\n",
    "\n",
    "In this exercise, you will familiarize yourself with various deep learning practices, including hyper-parameter tuning, regularization, and optimization techniques. You will apply these techniques within the context of a binary classification task designed to classify a given audio file as either speech or music. Please refer to the \"ex3_instructions.pdf\" file for a complete description of the problem and the applied techniques.\n",
    "\n",
    "Code Template\n",
    "\n",
    "To complete this assignment, you will progress through four different stages (tasks):\n",
    "\n",
    "**Task 1. Basic model architecture, training, and testing loops (5 points)**\n",
    "\n",
    "**Task 2. Fine-tuning practices (5 points)**\n",
    "\n",
    "**Task 3. Optimization practices (5 points)**\n",
    "\n",
    "**Task 4. Regularization practices (5 points)**\n",
    "\n",
    "### **Deliverables:** \n",
    "\n",
    "Please submit the completed notebooks below, along with all the requested trained models (.pth) for the corresponding subtasks.\n",
    "\n",
    "* ex3_train_NN.ipynb\n",
    "* 'base_model.pth'\n",
    "* 'relu_model.pth'\n",
    "* 'lr_model.pth'\n",
    "* 'shuffle_model.pth'\n",
    "* 'bs_model.pth'\n",
    "* 'SGD_model.pth'\n",
    "* 'normalized_model.pth'\n",
    "* 'pooled_model.pth'\n",
    "\n",
    "### **Data**\n",
    "\n",
    "The dataset used for this exercise consists of a collection of audio .wav files, each with a duration of 5 seconds. You can find and download the data from the Moodle page of the course (Exercise 3). The dataset is provided as a ~150 MB ZIP file on Moodle. Please download the data and extract it into the same folder as the exercise files, naming the folder \"dataset_ex3.\" The \"dataset_ex3\" folder includes \"speech_wav\" and \"music_wav\" folders, each containing audio files for speech and music, respectively.\n",
    "\n",
    "*Note:* Your dataset path should point to the \"dataset_ex3\" folder, which contains the \"speech\" and \"music\" sub-folders. Be mindful of any extra folder levels that may be created when extracting the \"dataset_ex3.zip\" file.\n",
    "\n",
    "After downloading the data and setting up the folders, you are ready to begin the exercise tasks. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cad2871-f7ff-4084-916d-19fd808753f6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ceceb9f536e4f43322186ce360d5e2bc",
     "grade": false,
     "grade_id": "cell-45f1c2af76b09749",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 1: Basic Model Design and Training (5 Points)\n",
    "\n",
    "In this task, you will design and train a basic model for a speech versus music audio classification task. The model takes an audio signal as input and outputs a class label predicting whether the audio signal is speech or music. This exercise uses a convolutional neural network model.\n",
    "\n",
    "Your task is to design the basic block of the neural model, write a training loop function to train the model for several epochs, and implement a validation function to test the model's performance on the validation set.\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "**Task 1.1: Design the model architecture** (2 points)\n",
    "\n",
    "**Task 1.2 & Task 1.3: Complete the training and validation loops** (2 points)\n",
    "\n",
    "**Successful run of the submitted 'base_model.pth'** (1 point)\n",
    "\n",
    "### Deliverables from this task:\n",
    "\n",
    "* 'base_model.pth'\n",
    "\n",
    "**Important**: Always use deep copy if you want to make your own config dictionary in the code. The exercise has already separate config placeholder for each task but you if you want to test with different config then make sure to use deep copy instead of direct initialization to another variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e291401-df35-4759-bf0f-6109febc6426",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_training = False   # You can set it to True if you want to run inference on your trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f7ec28-0850-4eb3-a73f-7f18a45babb4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af5a674227d8d2e73b157fd76732f431",
     "grade": true,
     "grade_id": "cell-7b0f342d5ac2e1ee",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e853b0-e064-4e34-a6eb-9d13b9202057",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f7fb51763c008a67fe74c8129b45fc5",
     "grade": false,
     "grade_id": "cell-e350d1b0e2c20bc0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389bd5fd-3a45-4ca8-b38c-a3841e14b4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Set random seeds for all libraries\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.cuda.manual_seed_all(1) \n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20ee04-3448-4f58-a4db-b45eb2291467",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9b3657c9847477acc023c07d46137dbd",
     "grade": false,
     "grade_id": "cell-8b4b189ba6aee857",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Select the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce756ba9-927f-49f0-9432-a8c5a0c79536",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7f8d25bdf07ddc1ec85c4f23e98397dd",
     "grade": false,
     "grade_id": "cell-3a10875707170318",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d0380-f424-42ee-83e6-c5c053ec58b2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ef7495bbf3dfca15975d6437209e0a4",
     "grade": false,
     "grade_id": "cell-417bbd087fc67322",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Add the data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde2f01-15cc-4e19-9a73-07a354391ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"dataset_ex3\" # you can change the path if you want to store the dataset somewhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4460ebe6-89f7-4e4a-a260-a6312acb9f4b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b13c030978f3f075350c78ac589f2cb6",
     "grade": true,
     "grade_id": "cell-7a05e8ea3f9468d7",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc4c64-34f1-4012-9982-6217138564f0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "55b06875bab3a9f774bf971c76c56be9",
     "grade": false,
     "grade_id": "cell-f7d981238aed90fa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The cell below defines a function for preparing a list of audio file paths from specified directories. It reads .wav files for each class (music and speech), shuffles the data, and splits it into training and validation sets based on the given validation split ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d1777-4c54-4b79-a4f4-0e74bfaff10b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a244a21c32aaa4b68afc50678b55ac3",
     "grade": false,
     "grade_id": "cell-6873f2001d3f2c75",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Dataset):\n",
    "    def __init__(self, mode):\n",
    "        super(DataGenerator, self).__init__()\n",
    "        self.dataset_path = path\n",
    "        \n",
    "        self.duration_per_file_in_s = 5 # crop the input audios to 5 seconds\n",
    "        self.sampling_rate=22050\n",
    "        self.samples_per_file = int(self.duration_per_file_in_s * self.sampling_rate)\n",
    "        \n",
    "        self.music_files = glob.glob(os.path.join(path, 'music_wav', '*.wav'))\n",
    "        self.speech_files = glob.glob(os.path.join(path, 'speech_wav', '*.wav'))\n",
    "\n",
    "        # Splitting files for train and test\n",
    "        music_split_idx = int(0.7 * len(self.music_files))\n",
    "        speech_split_idx = int(0.7 * len(self.speech_files))\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.music_files = self.music_files[:music_split_idx]\n",
    "            self.speech_files = self.speech_files[:speech_split_idx]\n",
    "        elif mode == 'test':\n",
    "            self.music_files = self.music_files[music_split_idx:]\n",
    "            self.speech_files = self.speech_files[speech_split_idx:]\n",
    "\n",
    "        # Combine all files for indexing\n",
    "        self.files = self.music_files + self.speech_files\n",
    "        self.labels = [0]*len(self.music_files) + [1]*len(self.speech_files)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        \n",
    "        file_path = self.files[item]\n",
    "        label = self.labels[item]\n",
    "\n",
    "        audio_data, sr = torchaudio.load(file_path)  # Returns a tensor with shape [channels, samples]\n",
    "        \n",
    "        # Resample if the sample rate is not 22050\n",
    "        if sr !=  self.sampling_rate:\n",
    "            resample_transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq= self.sampling_rate)\n",
    "            audio_data = resample_transform(audio_data)\n",
    "\n",
    "        if audio_data.size(0) > 1:  # Check if more than 1 channel\n",
    "            audio_data = torch.mean(audio_data, dim=0, keepdim=True)\n",
    "\n",
    "        if audio_data.size(1) > self.samples_per_file:\n",
    "            audio_data = audio_data[:, :self.samples_per_file]  # Truncate\n",
    "        else:\n",
    "            pad_length = self.samples_per_file - audio_data.size(1)\n",
    "            audio_data = torch.nn.functional.pad(audio_data, (0, pad_length))  # Pad at the end\n",
    "\n",
    "\n",
    "        return audio_data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166bdae-d892-4250-b05c-99b3319204de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95fc72523e6ac00625032679215b3085",
     "grade": false,
     "grade_id": "cell-c05d98e2f3351f61",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.1: Model Architecture\n",
    "\n",
    "In this task, you will design a **configurable convolutional neural network** and implement the model class based on the given instructions.  \n",
    "\n",
    "In the **`MyModel`** template below, which is designed to define the model class, fill in the blanks to complete the architecture according to the provided configuration dictionary.  \n",
    "\n",
    "The **`BasicBlock`** class serves as a fundamental building block for constructing convolutional networks. Each block consists of:  \n",
    "- A convolutional layer (`Conv1d`)  \n",
    "- An optional batch normalization layer  \n",
    "- A non-linear activation function (`ReLU` or `Tanh`)  \n",
    "- An optional dropout layer  \n",
    "\n",
    "At the end of the network, you will include a **Global Average Pooling** layer, a **Flatten** layer, and an **Output Activation (Sigmoid)** layer.  \n",
    "\n",
    "You are also given a configuration dictionary (`base_config`) that defines model hyperparameters, such as the number of blocks, channels, and layer properties:\n",
    "\n",
    "```python\n",
    "base_config = {\n",
    "    'nb_basic_blocks': int,\n",
    "    'conv_channels': list,\n",
    "    'kernel_size': int,\n",
    "    'stride': int,\n",
    "    'non_linearity': str,\n",
    "    'use_batchnorm': bool,\n",
    "    'use_dropout': bool,\n",
    "    'batch_size': int,\n",
    "    'shuffle': bool,\n",
    "    'optimizer_type': str,\n",
    "    'learning_rate': float,\n",
    "    'apply_pooling': bool\n",
    "}\n",
    "```\n",
    "**Hint:**  \n",
    "You will need to use a **loop** to create multiple `BasicBlocks` dynamically and append it into layers.  \n",
    "Use `nn.ModuleList()` to store these blocks so that PyTorch registers them as part of the model.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <figure>\n",
    "        <img src=\"neural_network_model.jpg\" alt=\"Model Architecture\" style=\"width:900px; height:auto;\"/>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "useful links:\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool1d.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f0994b-2f95-4a02-bf48-06b66fca4ae8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "88aab1abbbcdec9066626e68373a8d3b",
     "grade": false,
     "grade_id": "cell-30eb42e554cad534",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, non_linearity, apply_batchnorm, apply_dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.apply_batchnorm = apply_batchnorm\n",
    "        self.apply_dropout= apply_dropout\n",
    "        \n",
    "        self.conv_layer = nn.Conv1d(in_channels=in_channels, out_channels=out_channels,kernel_size=kernel_size, stride=stride)\n",
    "\n",
    "        if apply_batchnorm:\n",
    "            self.bn = nn.BatchNorm1d(out_channels)\n",
    "            \n",
    "        if non_linearity == \"ReLU\":\n",
    "            self.activation_fn = nn.ReLU()\n",
    "        elif non_linearity == \"Tanh\":\n",
    "            self.activation_fn = nn.Tanh()\n",
    "\n",
    "        if apply_dropout:\n",
    "            self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        if self.apply_batchnorm:\n",
    "            x = self.bn(x)     \n",
    "        x = self.activation_fn(x)  \n",
    "      \n",
    "        if self.apply_dropout:\n",
    "            x=self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, nb_basic_blocks, conv_channels, kernel_size, stride, non_linearity, apply_batchnorm, apply_dropout, apply_pooling):\n",
    "        super().__init__()\n",
    "        self.apply_pooling = apply_pooling\n",
    "        self.feature_extractor = None\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # your code here for initializing layers\n",
    "        # ---------------------------------------------------------------------\n",
    "        # In this task, you will design a flexible CNN model using the BasicBlock\n",
    "        # defined above. Each BasicBlock may include:\n",
    "        #   - Conv1d layer\n",
    "        #   - Optional BatchNorm1d\n",
    "        #   - Activation (ReLU or Tanh)\n",
    "        #   - Optional Dropout\n",
    "        #\n",
    "        # Follow the steps below to build the model:\n",
    "        #\n",
    "        # 1. nn.ModuleList() is initialized and stored in `self.layers`.\n",
    "        #\n",
    "        # 2. Use a loop to create the specified number of BasicBlocks:\n",
    "        #       - For the first block, set in_channels = 1\n",
    "        #       - For the remaining blocks, set in_channels = conv_channels[i-1]\n",
    "        #       - Set out_channels = conv_channels[i]\n",
    "        #       - Pass kernel_size, stride, non_linearity, apply_batchnorm,\n",
    "        #         and apply_dropout as parameters to BasicBlock.\n",
    "        #       - Append each BasicBlock to `self.layers`.\n",
    "        #\n",
    "        # 3. If apply_pooling = True:\n",
    "        #       - After each BasicBlock in the loop iteration (except the last one),\n",
    "        #         append a MaxPool1d layer to `self.layers`\n",
    "        #         with the same kernel_size and stride.\n",
    "        #\n",
    "        # 4. Once all layers are added, wrap them using nn.Sequential:\n",
    "        #       self.feature_extractor = nn.Sequential(*self.layers)\n",
    "        #\n",
    "        # 5. Add the remaining layers:\n",
    "        #       - A Global Average Pooling layer (AdaptiveAvgPool1d)\n",
    "        #       - A Flatten layer\n",
    "        #       - If apply_pooling = True, also include a Linear (fc) layer\n",
    "        #         that maps the last channel size to 1.\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # your code here for the forward pass\n",
    "        # ---------------------------------------------------------------------\n",
    "        # 1. Pass the input through the feature extractor (self.feature_extractor),\n",
    "        #    which consists of multiple BasicBlocks and optional pooling layers.\n",
    "        #\n",
    "        # 2. Apply the global average pooling layer to reduce the spatial dimension.\n",
    "        #\n",
    "        # 3. Flatten the pooled output to a vector using the flatten layer.\n",
    "        #\n",
    "        # 4. If apply_pooling = True, pass the output through the fully connected layer.\n",
    "        #\n",
    "        # 5. Finally, apply the Sigmoid activation to obtain the final output.\n",
    "        # ---------------------------------------------------------------------\n",
    "        #\n",
    "        # Hint:\n",
    "        # The order should always be:\n",
    "        # feature_extractor → global_avg_pool → flatten → (fc) → sigmoid\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        return x\n",
    "\n",
    "def get_num_trainable_parameters(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'The model has {num_params} trainable parameters.')\n",
    "    return num_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba070278-d52e-4654-a1ef-f54d7c09b2de",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b29819f1e3d0f81180cc7caeb0e78a2",
     "grade": false,
     "grade_id": "cell-6b674c8d5b57ed13",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.2: Define Loss Function and Optimizer\n",
    "\n",
    "In this task, you will define a helper function named **`loss_and_optimizer`** that initializes both the **loss function** and the **optimizer** for your model training.\n",
    "\n",
    "The function should accept the following parameters:\n",
    "- **`model`** – the neural network whose parameters need to be optimized  \n",
    "- **`optimizer_type`** – a string specifying the optimizer to use (e.g., `'Adam'` or `'SGD'`)  \n",
    "- **`learning_rate`** – a floating-point value specifying the optimizer’s learning rate  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2df590-dac7-4943-8d86-8ad144abb9ba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebef7344f2f54fd27b69e4465feeea00",
     "grade": false,
     "grade_id": "cell-08b0c9a4997eb798",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def loss_and_optimizer(model, optimizer_type, learning_rate):\n",
    "\n",
    "    # your code here\n",
    "    # 1. Define the loss function as Binary Cross-Entropy Loss.\n",
    "    # 2. Initialize the optimizer based on optimizer_type:\n",
    "    #    - If 'Adam': use Adam optimizer.\n",
    "    #    - If 'SGD': use SGD optimizer.\n",
    "    # 3. Return both the criterion and optimizer.\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850ac8a0-f584-4a42-b68d-5e3d4a697c08",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2ec844ea1798013c10562e44d01d1ea",
     "grade": false,
     "grade_id": "cell-06a29833e043fcef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train and Validate Base Model\n",
    " \n",
    "In this task you are required to complete the python dictionary for the neural network configuration. You need to fill the value for the following keys\n",
    "\n",
    "* conv_channels = 3\n",
    "  - First channel has 1 input layer and 32 output layers\n",
    "  - Second channel has 32 input layers and 32 output layers\n",
    "  - Third channel has 32 input layers and 1 output layer\n",
    "  **Remember**: The `conv_channels` key in base_config is a list that holds the the input and output channel layers as parameters for your implementation in **MyModel** class\n",
    "* optimizer_type = Adam,\n",
    "* learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8424cd7-6f32-409d-8ab6-d538ddec67ad",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3bfc778a694212af766bb311aac7b140",
     "grade": false,
     "grade_id": "cell-a478003cadbc62d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    'nb_basic_blocks': 3,\n",
    "    'conv_channels': None,\n",
    "    'kernel_size': 11,\n",
    "    'stride': 5,\n",
    "    'non_linearity': 'Tanh',\n",
    "    'use_batchnorm': False,\n",
    "    'use_dropout': False,\n",
    "    'batch_size': 2,\n",
    "    'shuffle': False,\n",
    "    'optimizer_type': None,\n",
    "    'learning_rate': None,\n",
    "    'apply_pooling': False\n",
    "}\n",
    "\n",
    "# your code here\n",
    "# update the base_config dictionary and replace None with instructions provided in the above section\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6083e47b-0f02-4e87-907a-4478ccfb0103",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f5704998fc05af02a715192415970e15",
     "grade": false,
     "grade_id": "cell-31460a27459e766e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd5fc4-c08d-43af-8686-c3e2caca0a02",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59b1bd2597cbd465336a63d3ea29e877",
     "grade": true,
     "grade_id": "cell-87ae909b7908359a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "\n",
    "base_test_config = copy.deepcopy(base_config)\n",
    "\n",
    "all_tests_successful = True\n",
    "model = MyModel(base_test_config[\"nb_basic_blocks\"], base_test_config[\"conv_channels\"], base_test_config[\"kernel_size\"], \n",
    "                base_test_config[\"stride\"], base_test_config[\"non_linearity\"], base_test_config[\"use_batchnorm\"], \n",
    "                base_test_config[\"use_dropout\"], base_test_config[\"apply_pooling\"]).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "conv1d_count = sum(1 for layer in model.modules() if isinstance(layer, nn.Conv1d))\n",
    "if conv1d_count != 3:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 1.2, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "# Check the output shape\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 1.2, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 12033\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 1.2, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f679953-69e6-483a-a632-80018751fffb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9dfab269b47495625a1c4cc2af759869",
     "grade": false,
     "grade_id": "cell-0a5888fc734d0fea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 1.3: Training and Validation Loops\n",
    "\n",
    "In this task, you will complete the training and validation loops by filling in the blanks according to the provided instructions. The training loop iterates over the dataset to train the model, while the validation loop evaluates the model's performance on a separate validation dataset. The validation loop is called within the training loop to assess the model's accuracy and loss after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c357c15-6f45-4a06-b920-79fa29770689",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c2c439226d7fb381b3177f8e313a3369",
     "grade": false,
     "grade_id": "cell-de7de1fd3d3515a8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def training_loop(nb_epochs, model, optimizer, loss_fn, train_dataloader, test_dataloader, verbose=True):\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    model.train()\n",
    "    for epoch in range(1, nb_epochs + 1):\n",
    "        start = time.time()\n",
    "        train_loss, correct_predictions = 0., 0.\n",
    "        num_samples = 0\n",
    "        for i, (input_batch, target_batch) in enumerate(train_dataloader):\n",
    "            # your code here for minibatch training\n",
    "            # 1. call batch data and labels and set them to the correct device\n",
    "            # 2. make the prediction on the data\n",
    "            # 3. calculate loss\n",
    "            # 4. set optimizer to zero grad\n",
    "            # 5. do backward pass\n",
    "            # 6. move the optimizer one step forward\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "\n",
    "            # accumulate correct prediction\n",
    "            correct_predictions += ((predictions.detach() >= 0.5).int() == target_batch.int()).sum().item() # number of correct predictions\n",
    "            train_loss += loss_train.item()\n",
    "\n",
    "        average_train_loss = train_loss/(i+1)\n",
    "        average_train_accuracy = correct_predictions/len(train_dataloader.dataset)\n",
    "\n",
    "        test_loss, test_accuracy = testing_loop(model, loss_fn, test_dataloader)\n",
    "\n",
    "        train_losses.append(average_train_loss)\n",
    "        val_losses.append(test_loss)\n",
    "        train_accuracies.append(average_train_accuracy)\n",
    "        val_accuracies.append(test_accuracy)\n",
    "\n",
    "        end = time.time()\n",
    "        epoch_time = round(end - start, 2)\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch}, train_loss {average_train_loss:.2f}, train_accuracy: {average_train_accuracy:.4f},',\n",
    "                  f'test_loss {test_loss:.2f}, test_accuracy: {test_accuracy:.4f}, time = {epoch_time}')\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d14aac-8767-47c8-88e9-a5c39a1fff02",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3281f081d681fa8db347f3cbffc916b8",
     "grade": false,
     "grade_id": "cell-45c3b61513ffd3c8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def testing_loop(model, loss_fn, test_dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss, correct_predictions = 0., 0.\n",
    "        for i, (input_batch, target_batch) in enumerate(test_dataloader):\n",
    "            # your code here for minibatch validation\n",
    "            # 1. set input_batch, target_batch to correct device\n",
    "            # 2. make the prediction on input_batch\n",
    "            # 3. calculate loss and add it to previous loss\n",
    "            # 4. obtain predicted class labels from predictions\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "            correct_predictions += ((predictions.detach() >= 0.5).int() == target_batch.int()).sum().item()\n",
    "            \n",
    "    # Average for all batches\n",
    "    average_loss = total_loss / (i + 1)  # Use i + 1 for the total number of batches\n",
    "    average_accuracy = correct_predictions / len(test_dataloader.dataset)  \n",
    "    \n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e40477-3a3c-4e6e-998d-529fe9071d23",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14deb6b2a89c3f3d48d9860781691364",
     "grade": false,
     "grade_id": "cell-80f4807eea0fd726",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_losses, val_losses, train_acc, val_acc, nb_epochs):\n",
    "    epochs = range(1, nb_epochs + 1)\n",
    "    best_val_acc = max(val_acc)\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # ---- Loss plot ----\n",
    "    ax1.plot(epochs, train_losses, label='Train', color='tab:blue')\n",
    "    ax1.plot(epochs, val_losses, label='Validation', color='tab:orange')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # ---- Accuracy plot ----\n",
    "    ax2.plot(epochs, train_acc, label='Train', color='tab:blue')\n",
    "    ax2.plot(epochs, val_acc, label='Validation', color='tab:orange')\n",
    "    ax2.set_title(f'Accuracy (Best: {best_val_acc:.2f})')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f35b78-eb4b-44df-9420-6f4a47b3e3e5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "488ed8b2fdbe9c155b0440e29df1bada",
     "grade": false,
     "grade_id": "cell-faa5efb13a01620c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the below cell to check the correctness of your solution for the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9c0b4-335a-4f8b-8aff-0cb311e72866",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5859b2b2df0b5168166474e81a31acd1",
     "grade": true,
     "grade_id": "cell-c9706ace9d6f2b1c",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests the training code\n",
    "all_tests_successful = True\n",
    "\n",
    "model = MyModel(base_test_config[\"nb_basic_blocks\"], base_test_config[\"conv_channels\"], base_test_config[\"kernel_size\"], \n",
    "                base_test_config[\"stride\"], base_test_config[\"non_linearity\"], base_test_config[\"use_batchnorm\"], \n",
    "                base_test_config[\"use_dropout\"], base_test_config[\"apply_pooling\"]).to(device)\n",
    "dummy_optim = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "dummy_loss_fn = nn.BCELoss().to(device)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "dummy_inputs = torch.rand(2, 1, 22000).to(device)  \n",
    "dummy_labels = torch.randint(0, 2, (2,)).to(device) \n",
    "\n",
    "dummy_dataset = torch.utils.data.TensorDataset(dummy_inputs, dummy_labels)\n",
    "dummy_dl_train = torch.utils.data.DataLoader(dummy_dataset, batch_size=2)  \n",
    "dummy_dl_val = torch.utils.data.DataLoader(dummy_dataset, batch_size=8)\n",
    "\n",
    "from unittest.mock import patch, MagicMock\n",
    "\n",
    "with patch('torch.Tensor.backward') as mock_backward, patch.object(dummy_optim, 'step') as mock_step:\n",
    "    training_loop(\n",
    "        nb_epochs=1,\n",
    "        model=model,\n",
    "        optimizer=dummy_optim,\n",
    "        loss_fn=dummy_loss_fn,\n",
    "        train_dataloader=dummy_dl_train,\n",
    "        test_dataloader=dummy_dl_val,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    if mock_backward.called:   # check if .backward() was called\n",
    "        pass\n",
    "    else:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 1.3, Visible test: You forgot to calculate the gradients.\")\n",
    "        raise AssertionError(\"You forgot to calculate the gradients.\")\n",
    "\n",
    "    if mock_step.called: # check if .step() is used\n",
    "        pass\n",
    "    else:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 1.3, Visible test: You forgot to update the weights.\")\n",
    "        raise AssertionError(\"You forgot to update the weights.\")\n",
    "    \n",
    "if all_tests_successful:\n",
    "    success_str = 'Good job! you can now proceed to train your model.'\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb2c86e-00ce-4eaf-931a-7b0e76351ab8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "97c2372511c0291b326968d3a7c86a3f",
     "grade": false,
     "grade_id": "cell-be3857f66f0a3bde",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_and_test_your_model(config, model_name, save_model):\n",
    "\n",
    "    train_dataset = DataGenerator(mode='train')\n",
    "    test_dataset = DataGenerator(mode='test')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=config['shuffle'])\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=config['shuffle'])\n",
    "\n",
    "    model = MyModel(config['nb_basic_blocks'], config['conv_channels'], config['kernel_size'], config['stride'],\n",
    "                    config['non_linearity'], config['use_batchnorm'], config['use_dropout'], config['apply_pooling'])\n",
    "    loss_fn, optimizer = loss_and_optimizer(model, config['optimizer_type'], config['learning_rate'])\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_fn=loss_fn.to(device)\n",
    "\n",
    "    nb_epochs = 100\n",
    "\n",
    "    if not skip_training:\n",
    "        train_losses, val_losses, train_acc, val_acc = training_loop(nb_epochs, model, optimizer, loss_fn, train_loader, test_loader)\n",
    "        if save_model:\n",
    "            torch.save(model.state_dict(), f'{model_name}.pth')\n",
    "            print(\"Your trained model is saved successfully!\")\n",
    "\n",
    "        plot_loss_accuracy(train_losses, val_losses, train_acc, val_acc, nb_epochs)\n",
    "        results = {\n",
    "            \"metrics\": {\n",
    "              \"train_losses\": train_losses,\n",
    "              \"validation_losses\": val_losses,\n",
    "              \"training_accuracy\": train_acc,\n",
    "              \"validation_accuracy\": val_acc\n",
    "            },\n",
    "            \"test_loader\": test_loader,\n",
    "            \"loss_fn\": loss_fn\n",
    "\n",
    "        }\n",
    "        return results\n",
    "    else:\n",
    "        model.load_state_dict(torch.load(f'{model_name}.pth', map_location=device))\n",
    "        print(\"Loaded weights from your saved model successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba39cab-54bb-448e-ab35-e9d4ce9bf49f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b9cd4deeda5fdfa9675250ec7048b7a4",
     "grade": false,
     "grade_id": "cell-632afee7f06bac0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train the Model\n",
    " \n",
    "We will refer to this model as the \"base_model.\" The code will save the model. You are required to submit the trained \"basic_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2a4537-bc2f-489e-8204-d1364f57fa0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b2cef2172f446c60876ebcbbbb5c768",
     "grade": false,
     "grade_id": "cell-69974db3a8ac874b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "results = train_and_test_your_model(base_config, 'base_model', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9cdb3-c1de-481a-8738-3d5b28efc0ac",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8c7e19988d4362ca9a7349afba11102",
     "grade": false,
     "grade_id": "cell-d02a5030881356fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the below cell to check correctness of your solution for the training and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53b941-6602-434b-ba61-9051150593c7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e1c4c0eb2f067ee367f21f757efd4653",
     "grade": false,
     "grade_id": "cell-0de54dbb185009a0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "all_tests_successful = True\n",
    "if not skip_training:\n",
    "    try:\n",
    "        # Test 1: Ensure training accuracy is within the correct range\n",
    "        max_tacc = max(results['metrics'][\"training_accuracy\"])\n",
    "        if not (0.4 <= max_tacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 1.3, Visible test: Training accuracy {max_tacc} is out of the expected range [0.4, 1].\")\n",
    "            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.4, 1].\")\n",
    "\n",
    "        # Test 2: Ensure accuracy is within the correct range\n",
    "        max_vacc = max(\n",
    "            results['metrics'][\"validation_accuracy\"])\n",
    "        if not (0.4 <= max_vacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 1.3, Visible test: Validation accuracy {max_vacc} is out of the expected range [0.4, 1].\")\n",
    "            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.4, 1].\")\n",
    "\n",
    "        if all_tests_successful:\n",
    "            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n",
    "\n",
    "    except AssertionError as e:\n",
    "        feedback_txt.append(f\"Task 1.3, Visible test failed: {e}\")\n",
    "        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e96130-afa5-4151-82f8-0140ab41233f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72499014828444c8cb964d7828e74b4a",
     "grade": true,
     "grade_id": "cell-307e993829722beb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df921f9e-c527-45af-9cf2-caa1b8f147ee",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eecae489fe93ca94bc3e295ac0e502da",
     "grade": false,
     "grade_id": "cell-268197b5a15a08c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 2: Hyperparameter Tuning (5 Points)\n",
    "\n",
    "In this task, you will enhance your model’s performance through targeted modifications to the architecture. The main objectives include optimizing performance on the training dataset. The goal is to implement various changes and observe how they affect the overall performance of the model in terms of training and validation results, the smoothness and stability of the training curves, the number of trainable parameters, and the training time.\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "\n",
    "**Task 2.1: Increase Convolution Channels** (1 point)\n",
    "\n",
    "    Goal: Modify the model to increase the number of convolution channels to 128 across three layers.\n",
    "\n",
    "**Task 2.2: Add One Convolution Layer** (1 point)\n",
    "\n",
    "    Goal: Add an additional convolution layer to the model, creating four layers with 32 intermediate channels.\n",
    "\n",
    "**Task 2.3: Adjust Kernel Sizes (Smaller)** (1 point)\n",
    "\n",
    "    Goal: Modify the kernel sizes to be smaller than those used in the base model.\n",
    "\n",
    "**Task 2.4: Adjust Kernel Sizes (Larger)** (1 point)\n",
    "\n",
    "    Goal: Modify the kernel sizes to be larger than those used in the base model.\n",
    "\n",
    "**Task 2.5: Change Non-linearities (ReLU)** (1 point)\n",
    "\n",
    "    Goal: Replace the current activation functions with ReLU.\n",
    "\n",
    "### Deliverables from this task:\n",
    "\n",
    "* 'relu_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f98121b-aecd-4116-bfc6-534515ece7b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af4c2d9d3bacc157e059b15a991096a7",
     "grade": false,
     "grade_id": "cell-1d13342c4b666297",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.1. Increase Convolution Channels\n",
    "\n",
    "Use the base model from Task 1, increasing the convolutional channels from 32 to 128. Fill in the blanks in the cell below as instructed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb65036-3a10-498d-85f3-0ec0c81a40ee",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c18f89d02ec16e57d8e4ac9c56ba116a",
     "grade": false,
     "grade_id": "cell-52f869716b6d4621",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "increased_channel_model_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(increased_channel_model_config, '128_channel_model', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f82fc75-e8c2-4412-ae4a-bc64f14b7fdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d1ae42f0f2681cc1ffcbe58ac1e934e0",
     "grade": true,
     "grade_id": "cell-8644dfbff88f3dac",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(increased_channel_model_config['nb_basic_blocks'], increased_channel_model_config[\"conv_channels\"],\n",
    "                increased_channel_model_config[\"kernel_size\"], increased_channel_model_config[\"stride\"], \n",
    "                increased_channel_model_config[\"non_linearity\"], increased_channel_model_config[\"use_batchnorm\"], \n",
    "                increased_channel_model_config[\"use_dropout\"], increased_channel_model_config[\"apply_pooling\"]).to(device)\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 3:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.1, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [128, 128, 1]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 2.1, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        \n",
    "# Check the output shape\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.1, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 183297\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.1, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dac6859-ebc1-45c8-884e-22c588c5fc10",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cd75611a90cb2208187da3dc0430dcbd",
     "grade": true,
     "grade_id": "cell-1bea1284dfe26ccb",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests for checking the performance of the trained model\n",
    "all_tests_successful = True\n",
    "if not skip_training:\n",
    "    try:\n",
    "    \n",
    "        # Test 1: Ensure training accuracy is within the correct range\n",
    "        max_tacc = max(results['metrics'][\"training_accuracy\"])\n",
    "        if not (0.5 <= max_tacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 2.1, Visible test: Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            \n",
    "        # Test 2: Ensure accuracy is within the correct range\n",
    "        max_vacc = max(results['metrics'][\"validation_accuracy\"])\n",
    "        if not (0.5 <= max_vacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 2.1, Visible test: Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "    \n",
    "        if all_tests_successful:\n",
    "            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n",
    "    \n",
    "    except AssertionError as e:\n",
    "        feedback_txt.append(f\"Task 2.1, Visible test: {e}\")\n",
    "        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83452bef-391c-4d45-8a1b-233b29f1a8a9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3ea91259f0f552d152af64543a8e2394",
     "grade": false,
     "grade_id": "cell-d60b87270f811b88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.2: Add One Convolution Layer\n",
    "\n",
    "Next, use the base model from Task 1 (32 channels), adding one extra the convolutional layers to have 4 convolutional layers with 32 intermediate filters (channels). Fill in the blanks in the cell below as instructed in the code.\n",
    "\n",
    "**Hint**: Add one more layer in the `conv_channel` key for the `increased_basic_block_config`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f7bd26-7e81-4bf8-9593-93f78e2c2d3a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "04bdccd7c364c482095b871d4ecddf3b",
     "grade": false,
     "grade_id": "cell-cd4e236620bba643",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "increased_basic_block_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(increased_basic_block_config, '4_basic_blocks', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe3df5f-6d1b-4b86-bf0c-1e45fbd4a6a6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2bb5cf63e7af54a7626385d228eb5fd",
     "grade": false,
     "grade_id": "cell-79662b93b2f23fe4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b851d6e-f421-412d-a59e-a905585ac742",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb420ad0d5c4ec17785d4e0b4d4fb798",
     "grade": true,
     "grade_id": "cell-0d9169afb9854179",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(increased_basic_block_config['nb_basic_blocks'], increased_basic_block_config[\"conv_channels\"],\n",
    "                increased_basic_block_config[\"kernel_size\"], increased_basic_block_config[\"stride\"], \n",
    "                increased_basic_block_config[\"non_linearity\"], increased_basic_block_config[\"use_batchnorm\"], \n",
    "                increased_basic_block_config[\"use_dropout\"], increased_basic_block_config[\"apply_pooling\"]).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 4:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.2, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [32, 32, 32, 1]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 2.2, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        \n",
    "# Check the output shape\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.2, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Check the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 23329\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.2, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    \n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb859057-bc4c-48fb-be8b-e27648f2dba1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c458f436942e25a11f998414ab673700",
     "grade": false,
     "grade_id": "cell-f32c3e2b00a24dc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.3: Adjust Kernel Sizes (Smaller)\n",
    "In this task, you will modify the base model by using smaller kernel sizes for the convolutional layers. Set the kernel size to 7 and the stride to 3 and keeping the same convolutional channels as your initial implemetation. Fill in the blanks in the code cell below as directed to apply these changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c836ac08-b6da-4aec-8a96-10fffe29381e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7bb774fb15da5bfbcfbaddabff6cec5",
     "grade": false,
     "grade_id": "cell-3e864d3b29cc8373",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "reduced_kernel_size_and_stride_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(reduced_kernel_size_and_stride_config, 'reduced_kernel_size_and_stride', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76308293-3d30-4892-a5fa-4fffa3e17682",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14d9c908934782e9f9d1d8ec65381992",
     "grade": true,
     "grade_id": "cell-e268d234cf77f40f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "\n",
    "model = MyModel(reduced_kernel_size_and_stride_config['nb_basic_blocks'], reduced_kernel_size_and_stride_config[\"conv_channels\"], \n",
    "                reduced_kernel_size_and_stride_config['kernel_size'], reduced_kernel_size_and_stride_config['stride'], \n",
    "                reduced_kernel_size_and_stride_config['non_linearity'], reduced_kernel_size_and_stride_config['use_batchnorm'], \n",
    "                reduced_kernel_size_and_stride_config['use_dropout'], reduced_kernel_size_and_stride_config['apply_pooling']).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 3:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.3, Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [32, 32, 1]  # Expected output channels for the three layers\n",
    "expected_kernels = [7, 7, 7]\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 2.3, Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "    # Check the kernel size\n",
    "    if layer.kernel_size[0] != expected_kernels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 2.3, Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")    \n",
    "# Check the output shape\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.3, Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 7681\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.3, Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91aacf7-50c1-4d69-9fa9-7dcc4b5b6428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "92fe5d97c30906afd248a85fe42c0d73",
     "grade": false,
     "grade_id": "cell-11ffed8cc29c0f71",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.4: Adjust Kernel Sizes (Larger)\n",
    "In this step, return to the base model config and modify the kernel sizes of the convolutional layers to be larger. Use kernels of size 22 and a stride of 11. Complete the cell below by filling in the blanks as specified in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39817f45-b79a-47e5-aacc-cae403781ad9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7b547a9ba749593eb3f45a6755d8e83",
     "grade": false,
     "grade_id": "cell-9cd7cb14774030ef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "increased_kernel_size_and_stride_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results =  train_and_test_your_model(increased_kernel_size_and_stride_config, 'increased_kernel_size_and_stride', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de407996-1c2c-4bb5-8a5a-cc88bacefe2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d33db8c15d3fbef801d32bab35be53d",
     "grade": false,
     "grade_id": "cell-6d8ed9bec09c65d6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c4ae79-9454-4a14-ada2-85653fb00b70",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f6e9a9d8d0f4fcad1d26c9437bc8840c",
     "grade": true,
     "grade_id": "cell-d789b5b8ee81acf9",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(increased_kernel_size_and_stride_config[\"nb_basic_blocks\"], increased_kernel_size_and_stride_config[\"conv_channels\"], \n",
    "                increased_kernel_size_and_stride_config['kernel_size'], increased_kernel_size_and_stride_config['stride'], \n",
    "                increased_kernel_size_and_stride_config[\"non_linearity\"], increased_kernel_size_and_stride_config[\"use_batchnorm\"],\n",
    "                increased_kernel_size_and_stride_config[\"use_dropout\"], increased_kernel_size_and_stride_config[\"apply_pooling\"]).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "expected_kernels = [22, 22, 22]\n",
    "\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 3:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.4, Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [32, 32, 1]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 2.4, Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "    # Check the kernel size\n",
    "    if layer.kernel_size[0] != expected_kernels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 2.4, Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have the expected kernel size of {expected_kernels[i]}. It has {layer.kernel_size}.\")    \n",
    "\n",
    "# Check the output shape\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.4, Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 24001\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 2.4, Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43477da6-ff59-4d46-8cfe-abc622a5df31",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50bd7bd4d2a93850d2e2c24bc3132cc1",
     "grade": false,
     "grade_id": "cell-07ebd599973e351d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 2.5: Change Non-linearities\n",
    "For this task, return to the base model and explore the impact of different non-linear activation functions on the model's performance.\n",
    "\n",
    "Re-create the base model by filling in the blanks in the cell below as instructed. Try using \"ReLU\" as an alternative activation function to see how that affect training and validation results.\n",
    "\n",
    "Save the model as **relu_model.pth** and submit it to Moodle along with your other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a91fb2-3e84-4611-a1e9-0ee783680ff3",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9956e1a46da5681e25eb893daf02029c",
     "grade": false,
     "grade_id": "cell-cd76eb212edfccfd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "relu_model_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(relu_model_config, 'relu_model', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee05b0fc-4b93-4983-a1ae-0dd9ff7f2d95",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec1f418fc38473398710e690b1a8d1a1",
     "grade": false,
     "grade_id": "cell-c8c285cc82bdb4f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d82cb-3808-47b4-a83a-5a981e065288",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ade90a68bacdbd2d8f3eb167b5467aa",
     "grade": true,
     "grade_id": "cell-95e5355cd0d610b1",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests for checking the performance of the trained model\n",
    "all_tests_successful = True\n",
    "model = MyModel(relu_model_config['nb_basic_blocks'], relu_model_config[\"conv_channels\"], relu_model_config['kernel_size'], \n",
    "                relu_model_config['stride'], relu_model_config['non_linearity'], relu_model_config['use_batchnorm'],\n",
    "                relu_model_config['use_dropout'], relu_model_config['apply_pooling']).to(device)\n",
    "if not skip_training:\n",
    "    try:\n",
    "        # Test 0: Ensure non-linearity is ReLU\n",
    "        basic_blocks = [layer for layer in model.modules() if isinstance(layer, BasicBlock)]\n",
    "        for i, block in enumerate(basic_blocks):\n",
    "            if not isinstance(block.activation_fn, nn.ReLU):\n",
    "                all_tests_successful = False\n",
    "                feedback_txt.append(f\"Task 2.5, BasicBlock {i + 1} does not use ReLU as the activation function.\")\n",
    "                raise AssertionError(f\"BasicBlock {i + 1} does not use ReLU as the activation function.\")\n",
    "    \n",
    "        # Test 1: Ensure training accuracy is within the correct range\n",
    "        max_tacc = max(results['metrics']['training_accuracy'])\n",
    "        if not (0.5 <= max_tacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 2.5, Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            \n",
    "        # Test 2: Ensure accuracy is within the correct range\n",
    "        max_vacc = max(results['metrics']['validation_accuracy'])\n",
    "        if not (0.5 <= max_vacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 2.5, Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "    \n",
    "        if all_tests_successful:\n",
    "            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n",
    "    \n",
    "    except AssertionError as e:\n",
    "        feedback_txt.append(f\"Task 2.5, {e}\")\n",
    "        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321cd7d-88e4-4acb-aac8-a4699a2e832a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5a3d82a35867a8430134ce4913c7dbd2",
     "grade": true,
     "grade_id": "cell-ffbf6eae987ebc69",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c04529-7774-43b0-88d3-6d04d35d4f60",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cd7901c3d1f55cf5de303cf1a532f5f3",
     "grade": false,
     "grade_id": "cell-60149db0f9f6712c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 3: Optimization Practices (5 Points)\n",
    "\n",
    "In this task, you will practice optimization techniques by experimenting with different optimizers, learning rates, and batch sizes. The main objectives include observing the performance and computational costs related to training time. Additionally, pay attention to the training curves: in deep learning, smoother training and validation curves are generally desirable, as they often indicate stable learning and consistent generalization.\n",
    "\n",
    "You can achieve smoother training curves by adjusting the learning rate and increasing the batch size. Another important factor to consider is batch variability through data shuffling. While data shuffling can enhance training, very high learning rates, combined with shuffling, can negatively impact the smoothness and stability of the training process, potentially leading to unstable learning.\n",
    "\n",
    "By completing this task, you will gain insights into how different optimization choices affect the training dynamics of your model.\n",
    "\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "\n",
    "**Task 3.1: Experiment with Adam optimizer with different learning rates** (1 point)\n",
    "\n",
    "    Goal: Increase the learning rate in each test to find the optimal learning rate.\n",
    "\n",
    "**Task 3.2: Experiment with Adam optimizer with different learning rates and shuffling data in each batch** (1 point)\n",
    "\n",
    "    Goal: Allow the data to be shuffled in each batch, increase the learning rate, and observe its effect.\n",
    "\n",
    "**Task 3.3: Experiment with Adam optimizer and different batch sizes** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of batch sizes on training stability and efficiency.\n",
    "\n",
    "**Task 3.4: Experiment with SGD optimizer with different learning rates and shuffling data in each batch** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of data variability and learning rate in the SGD optimizer.\n",
    "\n",
    "**Task 3.5: Experiment with SGD optimizer and batch normalization layers** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of batch normalization layers in the SGD optimizer.\n",
    "    \n",
    "\n",
    "### Deliverables from this task:\n",
    "\n",
    "* 'lr_model.pth'\n",
    "* 'shuffle_model.pth'\n",
    "* 'bs_model.pth'\n",
    "* 'SGD_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8653f5-6a84-480a-9239-3008162fe669",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2bde008734387b479fe8f0bf8e52245b",
     "grade": false,
     "grade_id": "cell-09f8bb5b3d20a5bc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.1: Experiment with Adam optimizer with different learning rates\n",
    "\n",
    "Use the settings applied in Task 1 for the base model:\n",
    "\n",
    "- `batch_size = 8`\n",
    "- `optimizer = \"Adam\"`\n",
    "\n",
    "Gradually increase the learning rate from 0.0001 (used in the base model) to observe its effect and find the optimal learning rate for this setting. In particular, test the following learning rates:\n",
    "\n",
    "- `lr = [0.0001, 0.001, 0.01, 0.1]`\n",
    "\n",
    "Save the best-performing model as **'lr_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Goal**: Identify how different learning rates impact model performance, stability, and training efficiency for the current setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61d249-7c17-4ff8-9d07-82d85f499f4b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0920b85fb6b7dabfe05bfcd224fc56c5",
     "grade": false,
     "grade_id": "cell-5545109c808f3795",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "optimizer_lr_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(optimizer_lr_config, 'lr_model', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5961dc2-e644-490f-b174-37de07a2998e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73c09df6a8cb4f61a8de69edff925385",
     "grade": true,
     "grade_id": "cell-4c63c220f514bf27",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fcd4d3-8f9a-4190-9d30-258a634cd538",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24bd87aa1010114153079a7976f8d068",
     "grade": false,
     "grade_id": "cell-2a2eb72c7577ead5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.2: Experiment with Adam optimizer with different learning rates and shuffling data in each batch \n",
    "\n",
    "Repeat the experiments from Task 3.1, this time with data shuffling enabled to increase batch variability (by setting `shuffle=True`). Test the following learning rates:\n",
    "\n",
    "- `lr = [0.0001, 0.001, 0.01, 0.1]`\n",
    "\n",
    "Save the best-performing model as **'shuffle_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Goal**: Identify how different learning rates impact model performance and stability in the presence of data variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17566f57-5eb7-46ba-9864-3e138253f37a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8b792ac868f4427b08e2251630e2496a",
     "grade": false,
     "grade_id": "cell-e1b9ab40a7607bda",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "shuffle_model_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(shuffle_model_config, 'shuffle_model', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b574e-6e33-408b-b500-dc137a357863",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "300c5d498c3790bedb67423ad32414b8",
     "grade": true,
     "grade_id": "cell-0b2d57cb7c5290a5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e446bb9b-2a60-4cfa-869a-c7f7eb959428",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f00f7f19ec0cdd262b1671df130aa09",
     "grade": false,
     "grade_id": "cell-1cc1f418104f7a72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.3: Experiment with Adam optimizer and different batch sizes\n",
    "\n",
    "Now, set the optimizer to Adam with `lr = 0.001` and disable data shuffling (`shuffle=False`). Try different batch sizes as specified below:\n",
    "\n",
    "- `bs = [4, 8, 16, 32]`\n",
    "\n",
    "Save the best-performing model as **'bs_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Optional:** You may repeat the experiments above with data shuffling enabled (`shuffle=True`) to observe the impact of batch size when data variability is increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a999288-0633-4858-a695-54e462618572",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1d989f12dc6582e7bfba87e4cc62518",
     "grade": false,
     "grade_id": "cell-70e75064e280599c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "shuffle_model_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(shuffle_model_config, 'bs_model', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9dcc7a-641b-43b1-bac3-ac078aeb4d75",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0a635ec57c865c547e5f8774da428d31",
     "grade": true,
     "grade_id": "cell-a2f1c3666e4ad7e7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b85675-6978-4e5b-b0de-15e0240cb347",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e77b52914e92ec19595004e261068180",
     "grade": false,
     "grade_id": "cell-d31d06e19f5dd50e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.4: Experiment with SGD optimizer with different learning rates and shuffling data in each batch\n",
    "\n",
    "Repeat the experiments from Task 3.2, keeping data shuffling enabled, and change the optimizer to SGD. Test the following learning rates:\n",
    "\n",
    "- `lr = [0.0001, 0.001, 0.01, 0.1]`\n",
    "\n",
    "Save the best-performing model as **'SGD_model.pth'** and submit it to Moodle along with your other files.\n",
    "\n",
    "**Goal**: Observe the different behaviors of Adam and SGD optimizers, and examine how the choice of optimizer impacts the selection and effect of learning rate on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e60fc9-03cb-4b62-8de5-dcc99e3c48be",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4205781c5382821b0fc61c92a6bcbddc",
     "grade": false,
     "grade_id": "cell-b9aeb28c3d136e59",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sgd_model_config = copy.deepcopy(shuffle_model_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(sgd_model_config, 'SGD_model', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15aaebe-82f4-406f-8247-f737b794e0ab",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9a4a4e4975d7f26748af34b249cd246b",
     "grade": true,
     "grade_id": "cell-04784ef159da0b85",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61946c0f-bd80-4d33-9167-259e5e9249b8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ccbccc137576be5ce84a47cb07400d87",
     "grade": false,
     "grade_id": "cell-c09d3ce8367c660b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 3.5: Experiment with SGD optimizer and normalization layers\n",
    "\n",
    "Next, test the effect of batch normalization layers on the SGD optimizer using the following setup for training:\n",
    "\n",
    "- Batch data shuffling disabled (`shuffle=False`)\n",
    "- `learning_rate = 0.01`\n",
    "- `batch_size = 16`\n",
    "- `optimizer_type = SGD`\n",
    "\n",
    "Use the base model, then add a batch normalization layer within each convolutional block. Place this layer directly after each convolution layer and before applying the non-linearity function, as indicated in the code.\n",
    "\n",
    "`Hint: set use_batchnorm to true`\n",
    "\n",
    "**useful link**: https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad8e5c-bae3-42d5-a2c1-1a71b2d39738",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "249775001dcff84a36b666fd9ff4a1a5",
     "grade": false,
     "grade_id": "cell-2d839e2965a855e3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "normalization_model_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(normalization_model_config, 'normalization_model', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc271839-67b7-4b70-bab8-6be108caada5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "05713f37ccda1138648dada4c62ce7d5",
     "grade": true,
     "grade_id": "cell-a936094e8faa2af5",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(normalization_model_config['nb_basic_blocks'], normalization_model_config[\"conv_channels\"], normalization_model_config['kernel_size'], \n",
    "                normalization_model_config['stride'], normalization_model_config['non_linearity'], normalization_model_config['use_batchnorm'], \n",
    "                normalization_model_config['use_dropout'], normalization_model_config['apply_pooling']).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "\n",
    "# Dictionary to hold the execution order of each BasicBlock's layers\n",
    "layer_execution_order = {}\n",
    "# Function to capture forward pass order of layers within each BasicBlock\n",
    "def track_execution_order(module, input, output, name):\n",
    "    layer_types = []\n",
    "    for sub_module in module.children():  # Iterate through layers within BasicBlock\n",
    "        layer_types.append(type(sub_module))\n",
    "    layer_execution_order[name] = layer_types\n",
    "\n",
    "# Register hooks on each BasicBlock to capture layer order in forward pass\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, BasicBlock):\n",
    "        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n",
    "\n",
    "# Run the model forward pass to trigger hooks\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Define the expected order of layer types for BasicBlock\n",
    "expected_order = [nn.Conv1d, nn.BatchNorm1d, nn.Tanh]  \n",
    "\n",
    "# Check if each BasicBlock followed the expected order\n",
    "for name, order in layer_execution_order.items():\n",
    "    # Modify expected_order based on the chosen activation in model\n",
    "    current_expected_order = expected_order[:]\n",
    "    activation_fn_type = type(model.layers[0].activation_fn)  # Get the actual activation type\n",
    "    current_expected_order[-1] = activation_fn_type\n",
    "    \n",
    "    if order != current_expected_order:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 3.5, Visible test: {name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]}.\"\n",
    "                           f\"but got {[cls.__name__ for cls in order]}.\")\n",
    "        raise AssertionError(\n",
    "            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "            f\"but got {[cls.__name__ for cls in order]}.\"\n",
    "        )\n",
    "\n",
    "# Check output shape and range for LogSoftmax\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 3.5, Visible test: Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "\n",
    "# Final success message if all tests pass\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4153a-597f-449f-9046-1b3f3f41e1d7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9c0f2b745b9cc0c4929ec6ce7b0d0d84",
     "grade": false,
     "grade_id": "cell-32584ff422efb7fb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Task 4: Regularization Practices (5 Points)\n",
    "\n",
    "In Task 2 you have tried differnt hyperparameter tuning techniques to increase the performance of the model on the training set. In this task, you will practice regularization techniques to help the model to generalize to unseen data, i.e., to increase the performance on the validation set. You are asked to start with a model that achieves a good performance on the training set compared to the base model. However, the number of trainable parameters in this model is large which leads to over-fitting as observed in the training and validation curves. Here we try different techniques to decreadse the number of trainable påarameters and to increase the model performance on validation set. \n",
    "\n",
    "\n",
    "### Summary of Tasks for This Stage\n",
    "\n",
    "\n",
    "**Task 4.1: Experiment with normalization layers** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of batch normalization in model generalization and training stability.\n",
    "\n",
    "**Task 4.2: Experiment with dropout layers** (1 point)\n",
    "\n",
    "    Goal: Observe the effect of dropout layers in model generalization.\n",
    "\n",
    "**Task 4.3: Experiment with efficient model architecture through pooling layers** (3 point)\n",
    "\n",
    "    Goal: Observe the effect of efficient model design through adjusting receptive field of layers.\n",
    "\n",
    "\n",
    "### Deliverables from this task:\n",
    "\n",
    "* 'normalized_model.pth'\n",
    "* 'pooled_model.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156b18f-757a-491b-b096-3c9f5003845c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0817b2b9e75055ed25c9adb268440201",
     "grade": false,
     "grade_id": "cell-11383a0618897151",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Model Architecture: \n",
    "\n",
    "Fill in the blanks as instructed in the code to design the model architecture similar to the base config model but with four convolutional blocks, where the number of filters (channels) in each convolutional layer is set to 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b026e892-1473-4260-9290-66f49f0c7487",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17ddd288a971b280d79b51cf257932b9",
     "grade": false,
     "grade_id": "cell-0b347d2067197eab",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "channel_128_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(channel_128_config, 'channel_128', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a8dc23-bb62-48b7-af04-5f238132a970",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a7e258d88c9bab0b3f13d219de575199",
     "grade": false,
     "grade_id": "cell-16a40bfe843509d1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to verify the correctness of your model architecture solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f93a5a-3c83-464c-8914-98d4be755071",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e3f03b96a407f547e86b88817806aa1",
     "grade": true,
     "grade_id": "cell-dc18644669c4f119",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(channel_128_config['nb_basic_blocks'], channel_128_config['conv_channels'], channel_128_config['kernel_size'],\n",
    "                channel_128_config['stride'], channel_128_config['non_linearity'],\n",
    "                channel_128_config['use_batchnorm'], channel_128_config['use_dropout'], channel_128_config['apply_pooling']).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "if conv1d_count != 4:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4, Visible test: Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "    raise AssertionError(f\"Expected 3 Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "expected_channels = [128, 128, 128, 1]  # Expected output channels for the three layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 4, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        \n",
    "# Check the output shape\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Chech the number of trainable parameters\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 363649\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e594404c-899f-4099-8faf-9175dd58a2b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc3bd5a0db96b93eccd360754256e15e",
     "grade": false,
     "grade_id": "cell-4c08af9dfcaefb0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 4.1: Experiment with normalization layers\n",
    "\n",
    "Add a batch normalization layer after the convolution layer and before applying the non-linearity function. Train the model, monitor the behavior of the training and validation curves, and observe how the normalization layer affects the validation performance.\n",
    "\n",
    "Save the model as **'normalized_model.pth'** and submit it to Moodle along with your other files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d39287-64da-42f2-ac4a-df5da5782ec0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6441941587e9ba73972b95932f431c98",
     "grade": false,
     "grade_id": "cell-33eff9feef979ca1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "normalized_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(normalized_config, 'normalized_model', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ebb530-6fb2-4b6d-8e09-844d192c1583",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b17d0e1bcd15b7b40f8a77a459105d6a",
     "grade": false,
     "grade_id": "cell-82a0dbe4e98847d8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to verify the correctness of your model architecture solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7208e6-abdc-43c1-a483-1e70e92020fb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da198b3b19fd38edfcdf22ad98877909",
     "grade": true,
     "grade_id": "cell-93c7f2a7fe78fad4",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(normalized_config['nb_basic_blocks'], normalized_config['conv_channels'], normalized_config['kernel_size'],\n",
    "                normalized_config['stride'], \"ReLU\",\n",
    "                normalized_config['use_batchnorm'], normalized_config['use_dropout'], normalized_config['apply_pooling']).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "\n",
    "# Dictionary to hold the execution order of each BasicBlock's layers\n",
    "layer_execution_order = {}\n",
    "# Function to capture forward pass order of layers within each BasicBlock\n",
    "def track_execution_order(module, input, output, name):\n",
    "    layer_types = []\n",
    "    for sub_module in module.children():  # Iterate through layers within BasicBlock\n",
    "        layer_types.append(type(sub_module))\n",
    "    layer_execution_order[name] = layer_types\n",
    "\n",
    "# Register hooks on each BasicBlock to capture layer order in forward pass\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, BasicBlock):\n",
    "        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n",
    "\n",
    "# Run the model forward pass to trigger hooks\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Define the expected order of layer types for BasicBlock\n",
    "expected_order = [nn.Conv1d, nn.BatchNorm1d, nn.ReLU]  \n",
    "\n",
    "# Check if each BasicBlock followed the expected order\n",
    "for name, order in layer_execution_order.items():\n",
    "    # Modify expected_order based on the chosen activation in model\n",
    "    current_expected_order = expected_order[:]\n",
    "    activation_fn_type = type(model.layers[0].activation_fn)  # Get the actual activation type\n",
    "    current_expected_order[-1] = activation_fn_type\n",
    "    \n",
    "    if order != current_expected_order:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 4.1, Visible test: {name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "                            f\"but got {[cls.__name__ for cls in order]}.\")\n",
    "        raise AssertionError(\n",
    "            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "            f\"but got {[cls.__name__ for cls in order]}.\"\n",
    "        )\n",
    "\n",
    "# Check output shape and range for LogSoftmax\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4.1, Visible test: Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "\n",
    "# Final success message if all tests pass\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04b0107-bfc8-40e1-a96e-f646430c3b8b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c144649060206ec1f284fa082c2813b",
     "grade": true,
     "grade_id": "cell-2cc5ef3d576fb0ec",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f70473-5bdc-488d-a477-84677e605dbb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a5fd7d7801be7277214bf7722dce25cd",
     "grade": false,
     "grade_id": "cell-38d80197be5fe59b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 4.2: Experiment with dropout layers\n",
    "\n",
    "Keep the normalization layers and add a dropout layer as the last layer of the convolutional block. \n",
    "\n",
    "Train the model, monitor the behavior of the training and validation curves, and observe how the dropout layers affects the validation performance.\n",
    "\n",
    "`Hint:` set dropout to True in your config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055e953-198e-4b05-a3ba-7ffda221c5bf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "40282513a5429f53af89a30698110928",
     "grade": false,
     "grade_id": "cell-1819914b7ffa862a",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "dropout_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(dropout_config, 'dropout_model', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e6192e-0db7-47a3-af36-a47c48ba1ae9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ee6d0484d0e47000d17ccf1b0c4ad824",
     "grade": false,
     "grade_id": "cell-ad683def87f23b99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Run the cell below to verify the correctness of your solution for the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdc87bd-94fe-484a-9ec4-ca2896dc81f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50af691c314a4d3d5253cfc373c319fe",
     "grade": true,
     "grade_id": "cell-e862c8ea453d2fbd",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(dropout_config['nb_basic_blocks'], dropout_config['conv_channels'], dropout_config['kernel_size'],\n",
    "                dropout_config['stride'], \"ReLU\",\n",
    "                dropout_config['use_batchnorm'], dropout_config['use_dropout'], dropout_config['apply_pooling']).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "\n",
    "# Dictionary to hold the execution order of each BasicBlock's layers\n",
    "layer_execution_order = {}\n",
    "\n",
    "# Function to capture forward pass order of layers within each BasicBlock\n",
    "def track_execution_order(module, input, output, name):\n",
    "    layer_types = []\n",
    "    for sub_module in module.children():  # Iterate through layers within BasicBlock\n",
    "        layer_types.append(type(sub_module))\n",
    "    layer_execution_order[name] = layer_types\n",
    "\n",
    "# Register hooks on each BasicBlock to capture layer order in forward pass\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, BasicBlock):\n",
    "        module.register_forward_hook(lambda mod, inp, out, n=name: track_execution_order(mod, inp, out, n))\n",
    "\n",
    "# Run the model forward pass to trigger hooks\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Define the expected order of layer types for BasicBlock with Dropout\n",
    "expected_order = [nn.Conv1d, nn.BatchNorm1d, nn.ReLU, nn.Dropout]  \n",
    "\n",
    "# Check if each BasicBlock followed the expected order\n",
    "for name, order in layer_execution_order.items():\n",
    "    # Modify expected_order based on the chosen activation in model\n",
    "    current_expected_order = expected_order[:]\n",
    "    activation_fn_type = type(model.layers[0].activation_fn)  # Get the actual activation type\n",
    "    current_expected_order[2] = activation_fn_type  # Ensure activation function is dynamically set\n",
    "\n",
    "    if order != current_expected_order:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 4.2, Visible test: {name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "                            f\"but got {[cls.__name__ for cls in order]}.\")\n",
    "        raise AssertionError(\n",
    "            f\"{name} layer order incorrect. Expected {[cls.__name__ for cls in current_expected_order]} \"\n",
    "            f\"but got {[cls.__name__ for cls in order]}.\"\n",
    "        )\n",
    "\n",
    "# Check output shape and range for LogSoftmax\n",
    "expected_shape = (1, 1)\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4.2, Visible test: Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, got {dummy_output.shape}.\")\n",
    "\n",
    "# Final success message if all tests pass\n",
    "if all_tests_successful:\n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cde04db-bc16-46c1-8e89-118068897028",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8a4e25f6e1fa43f2dee5aa23f179dfb",
     "grade": false,
     "grade_id": "cell-fd1f1415b1ac5fa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Task 4.3: Experiment with efficient model architecture through pooling layers\n",
    "\n",
    "As the final step, try to increase the model efficiency through a wiser design of the model architecture. In a deep learning model, while using a stack of convolutional blocks, it is common practice to use pooling layers between convolutional layers to decrease the dimension of the data and the resolution of deeper layers. This helps make the model lighter by reducing the number of trainable parameters and, at the same time, increases the model's performance by helping it focus on different feature types at different layers. For example, in the case of audio processing, the shallower layers can focus on finding variations in short time windows, while deeper layers can focus on detecting longer variations.\n",
    "\n",
    "In this task, you are asked to follow the same logic and modify the model architecture using pooling layers.\n",
    "\n",
    "Set the configuration for you config according to following:\n",
    "\n",
    "- kernel_size set to 10\n",
    "- stride set to 2\n",
    "- non_linearity to ReLU\n",
    "- normalization, dropout and pooling set to True\n",
    "- The conv_channel will be of size 3 with the following configuration:\n",
    "  - first layer: input channel: 1, output channel: 8\n",
    "  - second layer: input channel: 8, output channel: 16\n",
    "  - third layer: input channel: 16, output channel: 32\n",
    "\n",
    "    `Hint`: set the conv_channel to 8,16,32\n",
    "\n",
    "Save the trained model as **'pooled_model.pth'** and submit it to Moodle along with your other files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f202f8c2-b880-45f1-9df2-b2aace9a92c9",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e6e879f84c88cfeac2bcc203cbba29d",
     "grade": false,
     "grade_id": "cell-42da8962c7e599e0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "pooling_config = copy.deepcopy(base_config)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "results = train_and_test_your_model(pooling_config, 'pooled_model', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d54fe-0b6f-4379-b15f-53ba215c6073",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dbda612464e8102c2a945de83d26b118",
     "grade": true,
     "grade_id": "cell-3c72ba37ea31a699",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests here\n",
    "all_tests_successful = True\n",
    "model = MyModel(pooling_config['nb_basic_blocks'], pooling_config['conv_channels'], pooling_config['kernel_size'], \n",
    "                pooling_config['stride'], pooling_config['non_linearity'], pooling_config['use_batchnorm'], \n",
    "                pooling_config['use_dropout'], pooling_config['apply_pooling']).to(device)\n",
    "dummy_input = torch.randn(1, 1, 22000).to(device)\n",
    "dummy_output = model(dummy_input)\n",
    "\n",
    "# Count the number of Conv1d layers and check their channels\n",
    "conv1d_layers = [layer for layer in model.modules() if isinstance(layer, nn.Conv1d)]\n",
    "conv1d_count = len(conv1d_layers)\n",
    "\n",
    "# Test the number of Conv1d layers\n",
    "expected_conv_count = 3  # Number of Conv1d layers expected\n",
    "if conv1d_count != expected_conv_count:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4.3, Visible test: Expected {expected_conv_count} Conv1d layers, got {conv1d_count}.\")\n",
    "    raise AssertionError(f\"Expected {expected_conv_count} Conv1d layers, got {conv1d_count}.\")\n",
    "\n",
    "# Check expected output channels\n",
    "expected_channels = [8, 16, 32]  # Expected output channels for the three Conv1d layers\n",
    "\n",
    "for i, layer in enumerate(conv1d_layers):\n",
    "    if layer.out_channels != expected_channels[i]:\n",
    "        all_tests_successful = False\n",
    "        feedback_txt.append(f\"Task 4.3, Visible test: Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "        raise AssertionError(f\"Conv1d layer {i + 1} does not have {expected_channels[i]} filters (channels). It has {layer.out_channels} filters.\")\n",
    "\n",
    "# Check the output shape\n",
    "expected_shape = (1, 1)  # Expected output shape\n",
    "if dummy_output.shape != expected_shape:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4.3, Visible test: Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "    raise AssertionError(f\"Expected output shape {expected_shape}, but got {dummy_output.shape}.\")\n",
    "\n",
    "# Check the number of trainable parameters\n",
    "def get_num_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "num_params = get_num_trainable_parameters(model)\n",
    "expected_num_parameters = 6681  # Expected number of trainable parameters\n",
    "if num_params != expected_num_parameters:\n",
    "    all_tests_successful = False\n",
    "    feedback_txt.append(f\"Task 4.3, Visible test: Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "    raise AssertionError(f\"Expected number of trainable parameters {expected_num_parameters}, but got {num_params}.\")\n",
    "\n",
    "if all_tests_successful: \n",
    "    success_str = \"Good job! All visible tests passed! You can proceed further.\"\n",
    "    print(f\"\\033[92m{success_str}\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df123743-5aea-4ea0-b35d-371d76178479",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b04e1ce2a35d5e3c67c751f5dcb09105",
     "grade": true,
     "grade_id": "cell-8668903249ced1e8",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Visible tests for checking the performance of the trained model\n",
    "all_tests_successful = True\n",
    "if not skip_training:\n",
    "    try:  \n",
    "        # Test 1: Ensure training accuracy is within the correct range\n",
    "        max_tacc = max(results['metrics'][\"training_accuracy\"])\n",
    "        if not (0.5 <= max_tacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 4.3, Visible test: Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            raise AssertionError(f\"Training accuracy {max_tacc} is out of the expected range [0.5, 1].\")\n",
    "            \n",
    "        # Test 2: Ensure accuracy is within the correct range\n",
    "        max_vacc = max(results['metrics'][\"training_accuracy\"])\n",
    "        if not (0.5 <= max_vacc <= 1):\n",
    "            all_tests_successful = False\n",
    "            feedback_txt.append(f\"Task 4.3, Visible test: Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "            raise AssertionError(f\"Validation accuracy {max_vacc} is out of the expected range [0.5, 1].\")\n",
    "    \n",
    "        if all_tests_successful:\n",
    "            print(f\"\\033[92mAll visible tests for training and validation accuracy passed successfully!\\033[0m\")\n",
    "    \n",
    "    except AssertionError as e:\n",
    "        print(f\"\\033[91mTest failed: {e}\\033[0m\")\n",
    "\n",
    "else:\n",
    "    print(\"This visible test is applicable only when `skip_training` is set to `False`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf059382-1bc2-47d9-97e6-1638185e3dad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b564d08bfe96e324f01ca6087ecf50f",
     "grade": true,
     "grade_id": "cell-14856483d61954ab",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f4253-58a6-413c-844a-245080335d0f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfb948a55c0e0ec7eb16b9eb26e51425",
     "grade": true,
     "grade_id": "cell-9229ad2e11cd546c",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl_course_instructor]",
   "language": "python",
   "name": "conda-env-dl_course_instructor-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
